{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# SC4022 - Group Project\n",
    "## Network-Science Based Analysis of Collaboration Network of Data Scientists\n",
    "### Contributors:\n",
    "- Cholakov Kristiyan Kamenov\n",
    "- Chua Wee Siang, Fraser\n",
    "- Dhanyamraju Harsh Rao"
   ],
   "id": "b9f08c13fd67f454"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 0. Importing Libraries\n",
    "First, we will import the necessary libraries for the whole project."
   ],
   "id": "bc241c52f88369ef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T09:04:29.097902Z",
     "start_time": "2024-04-07T09:04:29.095439Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Importing the libraries\n",
    "import os  # Used for file operations\n",
    "import requests  # Used for making HTTP requests\n",
    "from bs4 import BeautifulSoup  # Used for parsing HTML\n",
    "import re  # Used for regular expressions\n",
    "import pandas as pd  # Used for data manipulation and analysis\n",
    "import numpy as np  # Used for numerical computations\n",
    "from tqdm import tqdm  # Used for progress bars\n",
    "import time  # Used for time operations\n",
    "import xml.etree.ElementTree as ET  # Used for parsing XML\n",
    "from ipytree import Tree, Node  # Used for displaying the XML tag hierarchy in a dynamic tree-like structure\n",
    "import json  # Used for JSON operations (used for the XML tag hierarchy)\n",
    "import ast # Used for converting string to list"
   ],
   "id": "cee1a7345143c343",
   "outputs": [],
   "execution_count": 213
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We will also set a random seed for reproducibility.",
   "id": "a35f7dc764c8e6f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T03:36:54.868371Z",
     "start_time": "2024-04-07T03:36:54.866810Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)"
   ],
   "id": "49b7cc2535487862",
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Data Collection\n",
    "In this part, we will be collecting the data from https://dblp.org (Computer Science Bibliography) connected to the given data scientists in DataScientists.xls file. For the sake of standardization and ease of use, we will first convert the xls file to csv file."
   ],
   "id": "8f2f06a6e2d20322"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-07T03:36:54.894981Z",
     "start_time": "2024-04-07T03:36:54.889979Z"
    }
   },
   "source": [
    "# Check if the Excel file is converted to a csv file\n",
    "if 'scientists_collab.csv' in os.listdir():\n",
    "    print('The CSV file already exists')\n",
    "else:\n",
    "    print('The Excel file has not been converted to a CSV file')\n",
    "    # Read the Excel file into a DataFrame\n",
    "    df = pd.read_excel('DataScientists.xls')\n",
    "    # Convert the DataFrame to a CSV file\n",
    "    df.to_csv('scientists_collab.csv', index=False)\n",
    "    print('The Excel file has been converted to a CSV file')\n",
    "\n",
    "# Read the converted CSV file into a DataFrame\n",
    "initial_df = pd.read_csv('scientists_collab.csv')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The CSV file already exists\n"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.1 Data Analysis on the Given Data\n",
    "Before we start collecting the data, we will have a look at the given data to understand the structure of the data."
   ],
   "id": "53152ac524aa6545"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T03:36:54.899798Z",
     "start_time": "2024-04-07T03:36:54.895908Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Displaying the number of rows and columns in the DataFrame\n",
    "print(f'The DataFrame has {initial_df.shape[0]} rows and {initial_df.shape[1]} columns')\n",
    "\n",
    "# Displaying the first 5 rows of the DataFrame\n",
    "initial_df.head(5)"
   ],
   "id": "836c177ee911d810",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DataFrame has 1220 rows and 5 columns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                   name        country  \\\n",
       "0          aaron elmore  united states   \n",
       "1   abdalghani abujabal        germany   \n",
       "2          abdul quamar  united states   \n",
       "3     abdulhakim qahtan    netherlands   \n",
       "4  abhijnan chakraborty        germany   \n",
       "\n",
       "                                 institution  \\\n",
       "0                      university of chicago   \n",
       "1                               amazon alexa   \n",
       "2                       ibm research almaden   \n",
       "3                         utrecht university   \n",
       "4  max planck institute for software systems   \n",
       "\n",
       "                                                dblp  expertise  \n",
       "0       https://dblp.org/pers/e/Elmore:Aaron_J=.html        NaN  \n",
       "1   https://dblp.org/pers/a/Abujabal:Abdalghani.html        NaN  \n",
       "2          https://dblp.org/pers/q/Quamar:Abdul.html        NaN  \n",
       "3                 https://dblp.org/pid/121/4198.html        NaN  \n",
       "4  https://dblp.org/pers/c/Chakraborty:Abhijnan.html        NaN  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>country</th>\n",
       "      <th>institution</th>\n",
       "      <th>dblp</th>\n",
       "      <th>expertise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aaron elmore</td>\n",
       "      <td>united states</td>\n",
       "      <td>university of chicago</td>\n",
       "      <td>https://dblp.org/pers/e/Elmore:Aaron_J=.html</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abdalghani abujabal</td>\n",
       "      <td>germany</td>\n",
       "      <td>amazon alexa</td>\n",
       "      <td>https://dblp.org/pers/a/Abujabal:Abdalghani.html</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abdul quamar</td>\n",
       "      <td>united states</td>\n",
       "      <td>ibm research almaden</td>\n",
       "      <td>https://dblp.org/pers/q/Quamar:Abdul.html</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abdulhakim qahtan</td>\n",
       "      <td>netherlands</td>\n",
       "      <td>utrecht university</td>\n",
       "      <td>https://dblp.org/pid/121/4198.html</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abhijnan chakraborty</td>\n",
       "      <td>germany</td>\n",
       "      <td>max planck institute for software systems</td>\n",
       "      <td>https://dblp.org/pers/c/Chakraborty:Abhijnan.html</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As we can see, the data contains the names of the data scientists, their countries and institutions. Also, the data contains the link to the DBLP page of the data scientist. We will later use this link to collect the data for each data scientist. But let's first perform some simple data analysis on the initial data.",
   "id": "c9cd52c8aab50d69"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T03:36:54.909233Z",
     "start_time": "2024-04-07T03:36:54.903611Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Display the number of unique values for each column\n",
    "print('Unique values for each column:')\n",
    "for column in initial_df.columns:\n",
    "    print(f'The column \"{column}\" has {initial_df[column].nunique()} unique values')\n",
    "    \n",
    "# Display the number of duplicated names\n",
    "print(f'\\nThere are {initial_df[\"name\"].duplicated().sum()} duplicated names')\n",
    "\n",
    "# Print 3 examples of row pairs with duplicated names in a string format\n",
    "print('\\nExamples of duplicated names:')\n",
    "# Get a random sample of 3 duplicated names\n",
    "for name in np.random.choice(initial_df['name'][initial_df['name'].duplicated()].unique(), 3):\n",
    "    print(f'Name: {name}')\n",
    "    # Print the rows with the duplicated name\n",
    "    for index in initial_df[initial_df['name'] == name].index:\n",
    "        print(f'-Country: {initial_df.loc[index, \"country\"]}, Institution: {initial_df.loc[index, \"institution\"]}, Link: {initial_df.loc[index, \"dblp\"]}')\n",
    "\n",
    "# Display the number of missing values for each column\n",
    "print('\\nMissing values for each column:')\n",
    "for column in initial_df.columns:\n",
    "    print(f'The column \"{column}\" has {initial_df[column].isnull().sum()} missing values')\n",
    "    "
   ],
   "id": "63cb518ba950acde",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values for each column:\n",
      "The column \"name\" has 1072 unique values\n",
      "The column \"country\" has 44 unique values\n",
      "The column \"institution\" has 704 unique values\n",
      "The column \"dblp\" has 1079 unique values\n",
      "The column \"expertise\" has 0 unique values\n",
      "\n",
      "There are 148 duplicated names\n",
      "\n",
      "Examples of duplicated names:\n",
      "Name: tingjian ge\n",
      "-Country: united states, Institution: university of massachusetts at lowell, Link: https://dblp.uni-trier.de/pers/g/Ge:Tingjian.html\n",
      "-Country: united states, Institution: university of massachusetts, lowell, Link: https://dblp.uni-trier.de/pers/g/Ge:Tingjian.html\n",
      "Name: sheng wang\n",
      "-Country: china, Institution: alibaba group, Link: https://dblp.org/pid/85/1868-11.html\n",
      "-Country: united states, Institution: new york university, Link: https://dblp.org/pid/85/1868.html\n",
      "-Country: china, Institution: wuhan university, Link: https://dblp.org/pid/85/1868-7.html\n",
      "Name: byron choi\n",
      "-Country: hong kong sar, Institution: hong kong baptist university, Link: https://dblp.uni-trier.de/pers/c/Choi:Byron.html\n",
      "-Country: hong kong sar, Institution: hong kong baptist university, hong kong, china, Link: https://dblp.uni-trier.de/pers/c/Choi:Byron.html\n",
      "\n",
      "Missing values for each column:\n",
      "The column \"name\" has 0 missing values\n",
      "The column \"country\" has 0 missing values\n",
      "The column \"institution\" has 3 missing values\n",
      "The column \"dblp\" has 0 missing values\n",
      "The column \"expertise\" has 1220 missing values\n"
     ]
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Looking at the simple data analysis, we can see that the data is fairly clean with only 3 missing values on the institution column. Also, we can observe that we have several duplicated names in the data. When we sample 3 examples of duplicated names and explore the rows with the duplicated names, we can see that the different rows present the same data scientist but with different institutions and countries. This is expected as the data scientists can work in different institutions and countries. We are sure the data scientist is the same as the rows have the same DBLP link. We can also see that the expertise column is empty (given as empty by the assignment).",
   "id": "66e5db77c71f0edd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.2 PIDs and Final URLs Collection\n",
    "Now, let's collect the PIDs and the Final Links (there may be some redirection using the links from the initial data) of the data scientists from the given data."
   ],
   "id": "443b5a384812e01c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T04:00:58.963034Z",
     "start_time": "2024-04-07T03:36:54.910064Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the lists to hold the PIDs and the final URLs\n",
    "pids = []\n",
    "final_urls = []\n",
    "\n",
    "# Define a variable to monitor for the links that caused errors\n",
    "errors_links = []\n",
    "\n",
    "# Check if we have already collected the PIDs and the final URLs\n",
    "if 'scientists_pids_urls.csv' in os.listdir():\n",
    "    print('The PIDs and the final URLs have already been collected')\n",
    "else:\n",
    "    # Iterate over the 'dblp' column in the initial DataFrame\n",
    "    for link in tqdm(initial_df['dblp']):\n",
    "        # Define an infinite loop to handle the Too Many Requests error\n",
    "        response = None\n",
    "        while True:\n",
    "            # Try sending a GET request to the link\n",
    "            response = requests.get(link)\n",
    "    \n",
    "            # If the status code is 429 (Too Many Requests), wait for 60 seconds before trying again\n",
    "            if response.status_code == 429:\n",
    "                print('Too many requests, sleeping for 60 seconds...')\n",
    "                time.sleep(60)\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        # If the status code is not 200 (some Error in fetching the data), append the link to the errors list\n",
    "        if response.status_code != 200:\n",
    "            errors_links.append(link)\n",
    "            pids.append('Error')\n",
    "            final_urls.append('Error')\n",
    "            continue\n",
    "            \n",
    "        # Get the final URL (after possible redirections)\n",
    "        final_url = response.url\n",
    "        # Define a regular expression pattern to extract the PID from the URL\n",
    "        pattern = r'pid/(.*).html'\n",
    "        # Find the PID in the URL\n",
    "        match = re.search(pattern, final_url)\n",
    "    \n",
    "        # If the PID is found in the URL\n",
    "        if match:\n",
    "            # Extract the PID\n",
    "            pid = match.group(1)\n",
    "            # Replace '/' with '-' for better naming\n",
    "            pid = pid.replace('/', '-')\n",
    "            # Append the PID to the list with PIDs\n",
    "            pids.append(pid)\n",
    "            # Append the final URL to the list with final URLs\n",
    "            final_urls.append(final_url)\n",
    "        else:\n",
    "            # Append an 'Error' to the list with PIDs to indicate that the PID was not found\n",
    "            pids.append('Error')\n",
    "            # Append an 'Error' to the list with final URLs to indicate that the final URL was not found\n",
    "            final_urls.append('Error')\n",
    "            # Append the link to the list with error links\n",
    "            errors_links.append(link)\n",
    "\n",
    "    # Check if there are any errors\n",
    "    if len(errors_links) == 0:\n",
    "        print('No errors occurred')\n",
    "    else:\n",
    "        print(f'The following links ({len(errors_links)}) caused errors:')\n",
    "        for el in errors_links:\n",
    "            print(el)"
   ],
   "id": "81445b046c7b7119",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1220/1220 [24:04<00:00,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following links (22) caused errors:\n",
      "https://dblp.org/pid/39/1380.html\n",
      "https://dblp.uni-trier.de/pers/c/Chakraborty:Anirban.html\n",
      "https://dblp.org/pid/92/2769.html\n",
      "https://dblp.org/pid/148/7268.html\n",
      "https://dblp.uni-trier.de/pers/b/Barbosa:Denilson.html\n",
      "https://dblp.org/pers/g/Georgakopoulos:Dimitrios.html\n",
      "https://dblp.org/pid/161/0102.html\n",
      "https://dblp.org/pers/m/Mansour:Essam.html\n",
      "https://dblp.org/pers/m/Mansour:Essam.html\n",
      "https://dblp.org/pid/284/0968.html\n",
      "https://dblp.org/pid/98/5721.html\n",
      "https://dblp.org/pers/j/Jung:Hyungsoo.html\n",
      "https://dblp.uni-trier.de/pers/p/Petrov:Ilia.html\n",
      "https://dblp.uni-trier.de/pers/p/Petrov:Ilia.html\n",
      "https://dblp.org/pers/h/Hui:Kai.html\n",
      "https://dblp.org/pers/w/Weidlich:Matthias.html\n",
      "https://dblp.uni-trier.de/pers/hd/n/Nikolic:Milos\n",
      "https://dblp.org/pers/z/Zhang:Ruqing.html\n",
      "https://dblp.dagstuhl.de/pid/y/TingYu.html\n",
      "https://dblp.dagstuhl.de/pid/y/TingYu.html\n",
      "https://dblp.uni-trier.de/pers/t/Tao:Yufei.html\n",
      "https://dblp.uni-trier.de/pers/t/Tao:Yufei.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As we can see, we encountered some errors while fetching the PIDs and the final URLs. Let's look at the heading (h1) of the pages to which the links lead to understand the errors.",
   "id": "f86bdfd1ed88822c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T04:01:16.784756Z",
     "start_time": "2024-04-07T04:00:58.964367Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define a list to hold the headings of the pages\n",
    "errors_links_headings = []\n",
    "\n",
    "# Check if the cleaning of the errors has already been done\n",
    "if 'scientists_pids_urls.csv' in os.listdir():\n",
    "    print('The cleaning of the errors has already been done')\n",
    "else:\n",
    "    for el in errors_links:\n",
    "        # Send a GET request to the error link\n",
    "        response = requests.get(el)\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        # Find the h1 tag\n",
    "        h1 = soup.find('h1')\n",
    "        # Append the heading to the list\n",
    "        errors_links_headings.append(h1.text if h1 else 'No h1 tag found')\n",
    "        \n",
    "    # Display the unique headings of the pages\n",
    "    print('Unique headings of the pages:')\n",
    "    for heading in set(errors_links_headings):\n",
    "        print('-' + heading)"
   ],
   "id": "4ee7a0c9023beb09",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique headings of the pages:\n",
      "-Error 410: Gone\n",
      "-Error 404: Not Found\n"
     ]
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The list of unique headings of the pages shows that the pages are not found (404 Not Found) or are gone (410 Gone). This is a common issue when fetching data from the web as the links may be outdated or the pages may have been removed. We will not be able to collect the data for these data scientists. Thus, we will remove these data scientists from the initial data.",
   "id": "4f262a3895445c0a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T04:01:16.812249Z",
     "start_time": "2024-04-07T04:01:16.786481Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check if the cleaning of the errors has already been done\n",
    "if 'scientists_pids_urls.csv' in os.listdir():\n",
    "    print('The cleaning of the errors has already been done')\n",
    "else:\n",
    "    # Create a new DataFrame by copying the initial DataFrame and adding the PIDs and final URLs\n",
    "    pids_urls_df = initial_df.copy()\n",
    "    pids_urls_df['pid'] = pids\n",
    "    pids_urls_df['final_url'] = final_urls\n",
    "    \n",
    "    # Remove the rows where the PID is 'Error' OR the final URL is 'Error'\n",
    "    pids_urls_df = pids_urls_df[(pids_urls_df['pid'] != 'Error') & (pids_urls_df['final_url'] != 'Error')]\n",
    "    \n",
    "    # Display the number of rows removed\n",
    "    print(f'{initial_df.shape[0] - pids_urls_df.shape[0]} rows were removed because their links were broken.\\n')\n",
    "    \n",
    "    # Drop the 'expertise' column as it is initially empty (will be filled later)\n",
    "    pids_urls_df = pids_urls_df.drop(columns='expertise')\n",
    "    \n",
    "    # Save the DataFrame to a CSV file\n",
    "    pids_urls_df.to_csv('scientists_pids_urls.csv', index=False)\n",
    "    \n",
    "    # Display the differance between the dimensions of the initial and the new DataFrame (from the CSV files)\n",
    "    print('Dimensions Comparison:')\n",
    "    print(f'New DataFrame: {pd.read_csv(\"scientists_pids_urls.csv\").shape[0]} x {pd.read_csv(\"scientists_pids_urls.csv\").shape[1]}')\n",
    "    print(f'Initial DataFrame: {pd.read_csv(\"scientists_collab.csv\").shape[0]} x {pd.read_csv(\"scientists_collab.csv\").shape[1]}')"
   ],
   "id": "e0e1c2c6fcded36b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 rows were removed because their links were broken.\n",
      "\n",
      "Dimensions Comparison:\n",
      "New DataFrame: 1198 x 6\n",
      "Initial DataFrame: 1220 x 5\n"
     ]
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "From now on, we will be using the newly created DataFrame with the PIDs and the final URLs (scientists_collab_pids_urls.csv) that we just cleaned from the broken links.",
   "id": "6778b3fba9c865f7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.3 Data Cleaning\n",
    "Now, we will explore the new cleaned data as there may still be some other issues with it."
   ],
   "id": "d02bff8f8de18fc4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T04:01:16.819775Z",
     "start_time": "2024-04-07T04:01:16.813259Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check the number of duplicated PIDs\n",
    "pids_urls_df = pd.read_csv('scientists_pids_urls.csv')\n",
    "print(f'The number of duplicated PIDs is {pids_urls_df[pids_urls_df.duplicated(subset=\"pid\", keep=False)].shape[0]}')"
   ],
   "id": "2e77f8dd34a63ea9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of duplicated PIDs is 278\n"
     ]
    }
   ],
   "execution_count": 76
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As we can see, we still have the duplicated PIDs in the data. Now, we will clean the rows with the duplicated PIDs and keep the unique data from them (country, institution, etc.).",
   "id": "d7e40b858cdef0ab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T04:28:37.497691Z",
     "start_time": "2024-04-07T04:28:37.479570Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Group the duplicated PIDs rows by the PID and aggregate the values\n",
    "duplicated_pids = pids_urls_df[pids_urls_df.duplicated(subset='pid', keep=False)].groupby('pid').agg(lambda x: set(x))\n",
    "# If the value for the column is a set of length = 1, get the first element of the set\n",
    "for column in duplicated_pids.columns:\n",
    "    duplicated_pids[column] = duplicated_pids[column].apply(lambda x: x.pop() if len(x) == 1 else x)\n",
    "\n",
    "# Display the number of rows with column values as sets for each column\n",
    "print('Number of rows with column values as sets for each column:')\n",
    "for column in duplicated_pids.columns:\n",
    "    print(f'-The column \"{column}\" has {duplicated_pids[duplicated_pids[column].apply(lambda x: isinstance(x, set))].shape[0]} rows with values as sets')\n",
    "    \n",
    "# Print the set values for the 'dblp' and 'final_url' columns\n",
    "print('\\nThe rows with set values in the \"dblp\" and \"final_url\" columns:')\n",
    "for index, row in duplicated_pids.iterrows():\n",
    "    # Check if the 'dblp' or 'final_url' columns have values as sets\n",
    "    if isinstance(row['dblp'], set) or isinstance(row['final_url'], set):\n",
    "        print(f'PID: {index}')\n",
    "        if isinstance(row['dblp'], set):\n",
    "            print(f'  -The \"dblp\" column has set of URLs: {row[\"dblp\"]}')\n",
    "        if isinstance(row['final_url'], set):\n",
    "            print(f'  -The \"final_url\" column has set of URLs: {row[\"final_url\"]}')"
   ],
   "id": "843048c78e19c08d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with column values as sets for each column:\n",
      "-The column \"name\" has 10 rows with values as sets\n",
      "-The column \"country\" has 21 rows with values as sets\n",
      "-The column \"institution\" has 95 rows with values as sets\n",
      "-The column \"dblp\" has 9 rows with values as sets\n",
      "-The column \"final_url\" has 5 rows with values as sets\n",
      "\n",
      "The rows with set values in the \"dblp\" and \"final_url\" columns:\n",
      "PID: 33-6315-1\n",
      "  -The \"dblp\" column has set of URLs: {'https://dblp.uni-trier.de/pid/33/6315-1.html', 'https://dblp.org/pid/33/6315-1.html'}\n",
      "  -The \"final_url\" column has set of URLs: {'https://dblp.uni-trier.de/pid/33/6315-1.html', 'https://dblp.org/pid/33/6315-1.html'}\n",
      "PID: 49-8316\n",
      "  -The \"dblp\" column has set of URLs: {'https://dblp.org/pers/k/Koutris:Paraschos.html', 'https://dblp.org/pid/49/8316.html'}\n",
      "PID: c-VassilisChristophides\n",
      "  -The \"dblp\" column has set of URLs: {'https://dblp.uni-trier.de/pid/c/VassilisChristophides.html', 'https://dblp.org/pid/c/VassilisChristophides.html'}\n",
      "  -The \"final_url\" column has set of URLs: {'https://dblp.uni-trier.de/pid/c/VassilisChristophides.html', 'https://dblp.org/pid/c/VassilisChristophides.html'}\n",
      "PID: f-GeorgeHLFletcher\n",
      "  -The \"dblp\" column has set of URLs: {'https://dblp.org/pid/f/GeorgeHLFletcher.html', 'https://dblp.uni-trier.de/pid/f/GeorgeHLFletcher.html'}\n",
      "  -The \"final_url\" column has set of URLs: {'https://dblp.org/pid/f/GeorgeHLFletcher.html', 'https://dblp.uni-trier.de/pid/f/GeorgeHLFletcher.html'}\n",
      "PID: p-IppokratisPandis\n",
      "  -The \"dblp\" column has set of URLs: {'https://dblp.org/pid/p/IppokratisPandis.html', 'https://dblp.uni-trier.de/pid/p/IppokratisPandis.html'}\n",
      "  -The \"final_url\" column has set of URLs: {'https://dblp.org/pid/p/IppokratisPandis.html', 'https://dblp.uni-trier.de/pid/p/IppokratisPandis.html'}\n",
      "PID: p-NeoklisPolyzotis\n",
      "  -The \"dblp\" column has set of URLs: {'https://dblp.org/pid/p/NeoklisPolyzotis.html', 'https://dblp.org/pers/p/Polyzotis:Neoklis.html'}\n",
      "PID: p-OdysseasPapapetrou\n",
      "  -The \"dblp\" column has set of URLs: {'https://dblp.org/pid/p/OdysseasPapapetrou.html', 'https://dblp.org/pers/p/Papapetrou:Odysseas.html'}\n",
      "PID: t-AnthonyKHTung\n",
      "  -The \"dblp\" column has set of URLs: {'https://dblp.org/pers/t/Tung:Anthony_K=_H=.html', 'https://dblp.org/pid/t/AnthonyKHTung.html'}\n",
      "PID: w-StevenEuijongWhang\n",
      "  -The \"dblp\" column has set of URLs: {'https://dblp.uni-trier.de/pid/w/StevenEuijongWhang.html', 'https://dblp.org/pid/w/StevenEuijongWhang.html'}\n",
      "  -The \"final_url\" column has set of URLs: {'https://dblp.uni-trier.de/pid/w/StevenEuijongWhang.html', 'https://dblp.org/pid/w/StevenEuijongWhang.html'}\n"
     ]
    }
   ],
   "execution_count": 93
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As we can see, several rows have values as sets for some columns. This means that they have multiple values for the same column. We can easily explain this for the columns: 'country', 'institution', 'name' as the data scientists can be part of multiple institutions in different countries and their names can be written in different ways. But for the columns: 'dblp' and 'final_url', we should have only one value for each row. After inspecting the provided links, we can observe they lead to pages with the same data. We will keep the link that starts with 'https://dblp.org/pid/' (to maintain consistency) and remove the rest of the links. We will perform the same operation for the 'final_url' column and not for the 'dblp' column as the 'final_url' will be used for the data collection, while the 'dblp' can just store the different links to the same page.",
   "id": "378d90090e29a906"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T04:31:11.186208Z",
     "start_time": "2024-04-07T04:31:11.181284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# For every row where the 'final_url' column is a set, keep the link that starts with 'https://dblp.org/pid/' and remove the rest of the links\n",
    "for index, row in duplicated_pids[duplicated_pids['final_url'].apply(lambda x: isinstance(x, set))].iterrows():\n",
    "    # Define a variable to store if such a link is found\n",
    "    found = False\n",
    "    for link in row['final_url']:\n",
    "        if link.startswith('https://dblp.org/pid/'):\n",
    "            duplicated_pids.loc[index, 'final_url'] = link\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        print(f'No link that starts with \"https://dblp.org/pid/\" was found for the PID: {index}')\n",
    "\n",
    "# Print the number of rows with 'final_url' as a set\n",
    "print(f'The number of rows with \"final_url\" as a set is {duplicated_pids[duplicated_pids[\"final_url\"].apply(lambda x: isinstance(x, set))].shape[0]}')"
   ],
   "id": "3f27729b4257c45e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of rows with \"final_url\" as a set is 0\n"
     ]
    }
   ],
   "execution_count": 97
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "After we have cleaned the duplicated data, we will append the cleaned data for the duplicated PIDs to the non-duplicated data.",
   "id": "950e9be4273c1c9c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T04:37:25.272707Z",
     "start_time": "2024-04-07T04:37:25.264397Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define a DataFrame to hold the non-duplicated PIDs\n",
    "non_duplicated_pids = pids_urls_df.drop_duplicates(subset='pid', keep=False)\n",
    "\n",
    "# Reset the index of duplicated_pids DataFrame\n",
    "duplicated_pids_reset = duplicated_pids.reset_index()\n",
    "\n",
    "# Append the cleaned duplicated PIDs to the non-duplicated PIDs\n",
    "cleaned_pids_urls_df = pd.concat([non_duplicated_pids, duplicated_pids_reset]).sort_values(by='pid').reset_index(drop=True)\n",
    "\n",
    "# Display the number of rows removed\n",
    "print(f'{pids_urls_df.shape[0] - cleaned_pids_urls_df.shape[0]} rows were removed because they were duplicates in terms of PIDs')\n",
    "\n",
    "# Display if there are still duplicated PIDs in the DataFrame\n",
    "print(f'The number of duplicated PIDs is {cleaned_pids_urls_df[cleaned_pids_urls_df.duplicated(subset=\"pid\", keep=False)].shape[0]}')\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "cleaned_pids_urls_df.to_csv('scientists_clean.csv', index=False)"
   ],
   "id": "69478c3f4a0a3540",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146 rows were removed because they were duplicates in terms of PIDs\n",
      "The number of duplicated PIDs is 0\n"
     ]
    }
   ],
   "execution_count": 107
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As we can see, we have successfully cleaned the data from the duplicated PIDs. Now, we will be using the cleaned data for the actual data collection from the DBLP pages.",
   "id": "93e3dc81cb4594b3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.4 Data Collection from DBLP\n",
    "After a closer inspection of the DBLP pages, we have identified that the data for each scientist is stored in an XML format file that can be accessed by just replacing the '.html' extension with '.xml' for the 'final_url' link. Now, let's build the DataFrame with the links to the XML files for each scientist."
   ],
   "id": "f99c220562d002c4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T04:43:56.040370Z",
     "start_time": "2024-04-07T04:43:56.029207Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Read the cleaned CSV file\n",
    "cleaned_pids_urls_df = pd.read_csv('scientists_clean.csv')\n",
    "\n",
    "# Creating a new column to hold the XML links\n",
    "cleaned_pids_urls_df['xml'] = cleaned_pids_urls_df['final_url'].apply(lambda x: x.replace('.html', '.xml'))\n",
    "\n",
    "# Check if all the XML links are ending with '.xml'\n",
    "if cleaned_pids_urls_df['xml'].apply(lambda x: x.endswith('.xml')).all():\n",
    "    print('All the XML links are ending with \".xml\"')\n",
    "    # Save the DataFrame to a CSV file\n",
    "    cleaned_pids_urls_df.to_csv('scientists_xml.csv', index=False)\n",
    "    print('The XML links have been added to the DataFrame and saved to a CSV file')\n",
    "else:\n",
    "    print('Not all the XML links are ending with \".xml\"')"
   ],
   "id": "20436eabffbefc70",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the XML links are ending with \".xml\"\n",
      "The XML links have been added to the DataFrame and saved to a CSV file\n"
     ]
    }
   ],
   "execution_count": 108
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, we have the DataFrame with the XML links for each scientist. We will use these links to collect the data for each scientist.",
   "id": "7137dc5d0c54cf57"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T05:01:00.223790Z",
     "start_time": "2024-04-07T04:46:46.529103Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get the DataFrame with the XML links from the CSV file\n",
    "xml_df = pd.read_csv('scientists_xml.csv')\n",
    "\n",
    "# Define a variable to store the error count that may occur during the data collection\n",
    "error_cnt = 0\n",
    "\n",
    "# Iterate over the DataFrame rows\n",
    "for index, row in tqdm(xml_df.iterrows(), total=xml_df.shape[0]):\n",
    "    # Check if the directory for the XML files exists, if not create it\n",
    "    if 'xml_files' not in os.listdir():\n",
    "        os.mkdir('xml_files')\n",
    "    \n",
    "    # Check if the file exists, skip the file if it exists\n",
    "    if f'{row[\"pid\"]}.xml' in os.listdir('xml_files'):\n",
    "        print(f'The file {row[\"pid\"]}.xml already exists')\n",
    "        continue\n",
    "    \n",
    "    while True:\n",
    "        # Send a GET request\n",
    "        response = requests.get(row['xml'])\n",
    "        \n",
    "        # If the status code is 429 (Too Many Requests), wait for 60 seconds before trying again\n",
    "        if response.status_code == 429:\n",
    "            print('Too many requests, sleeping for 60 seconds...')\n",
    "            time.sleep(60)\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    # Print Error if the status code is not 200\n",
    "    if response.status_code != 200:\n",
    "        error_cnt += 1\n",
    "        print(f'Error: {response.status_code}')\n",
    "        print(f'Error for {row[\"pid\"]}: {row[\"xml\"]}')\n",
    "        continue\n",
    "    \n",
    "    # Save the content to a file (create a new file for each PID)\n",
    "    with open(f'xml_files/{row[\"pid\"]}.xml', 'wb') as file:\n",
    "        file.write(response.content)\n",
    "\n",
    "# Print the number of errors\n",
    "print(f'{error_cnt} errors were encountered')"
   ],
   "id": "f4c8c2bc5a2fdb2c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1052/1052 [14:13<00:00,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 errors were encountered\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 110
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, we have collected the data for each scientist in an XML format file. We will have to explore the data to understand how to extract the necessary information for the data scientists.",
   "id": "f264463ef319eb95"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T05:22:23.362247Z",
     "start_time": "2024-04-07T05:22:21.476105Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def traverse_tree(element, tag_tree):\n",
    "    \"\"\"Recursive function to traverse the XML tree and build the hierarchy of tags.\"\"\"\n",
    "    for child in element:\n",
    "        if child.tag not in tag_tree:\n",
    "            tag_tree[child.tag] = {}\n",
    "        traverse_tree(child, tag_tree[child.tag])\n",
    "\n",
    "tag_tree = {}\n",
    "\n",
    "# Iterate over all XML files in the directory\n",
    "for filename in os.listdir('xml_files'):\n",
    "    if filename.endswith('.xml'):\n",
    "        # Parse the XML file\n",
    "        tree = ET.parse(os.path.join('xml_files', filename))\n",
    "        # Get the root element\n",
    "        root = tree.getroot()\n",
    "        # Traverse the XML tree and build the hierarchy of tags\n",
    "        traverse_tree(root, tag_tree)\n",
    "\n",
    "def build_tree(data, parent=None):\n",
    "    if type(data) is dict:\n",
    "        for key, value in data.items():\n",
    "            node = Node(key)\n",
    "            parent.add_node(node)\n",
    "            build_tree(value, node)\n",
    "    elif type(data) is list:\n",
    "        for index, value in enumerate(data):\n",
    "            node = Node(str(index))\n",
    "            parent.add_node(node)\n",
    "            build_tree(value, node)\n",
    "    else:\n",
    "        node = Node(str(data))\n",
    "        parent.add_node(node)\n",
    "\n",
    "# Load your JSON data\n",
    "data = json.loads(json.dumps(tag_tree, indent=4))\n",
    "\n",
    "# Create the root node\n",
    "root = Node(\"dblpperson\")\n",
    "# Build the tree\n",
    "build_tree(data, root)\n",
    "# Create a Tree instance and add the root node\n",
    "tree = Tree()\n",
    "tree.add_node(root)\n",
    "\n",
    "# Display the tree\n",
    "tree"
   ],
   "id": "85ddf107b9c4c2a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tree(nodes=(Node(name='dblpperson', nodes=(Node(name='person', nodes=(Node(name='author'), Node(name='note'), …"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4c1c9148e6cf431d9574122239c82a5f"
      }
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 124
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Exploring the XML tag hierarchy and referring to the https://dblp.org/xml/docu/dblpxml.pdf document (description of the DBLP XML format), we can define the tags that we will use to extract the necessary information for the data scientists. We will use the following tags:",
   "id": "8ed8f3f9a88370fc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T05:48:42.472770Z",
     "start_time": "2024-04-07T05:48:42.469943Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the tags that capture the publications\n",
    "publication_tags = ['inproceedings', 'article', 'incollection', 'book', 'proceedings', 'phdthesis', 'data', 'www']\n",
    "\n",
    "# Define the tags that capture the title of the publication\n",
    "title_tags = ['title']\n",
    "\n",
    "# Define the tags that caputre the year of the publication\n",
    "year_tags = ['year']\n",
    "\n",
    "# Define the tags that capture the authors of the publication\n",
    "author_tags = ['author', 'editor']"
   ],
   "id": "e6693d8bdfa48d23",
   "outputs": [],
   "execution_count": 127
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "After we have collected and explored the data for each scientist, we can now proceed to the conversion of the XML files to DataFrames.",
   "id": "a48587531d224cc5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.5 Data Conversion\n",
    "First, we have to create a DataFrame for all the publications of the data scientists. These publications will represent the links between the data scientists. We will also use this DataFrame to store information about the publications (title, year, etc.)."
   ],
   "id": "ed1be5fbcdef0703"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T08:55:18.434744Z",
     "start_time": "2024-04-07T08:55:00.259957Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize an empty list to store the data for each paper\n",
    "papers = []\n",
    "\n",
    "# Define the given authors\n",
    "our_authors = pd.read_csv('scientists_clean.csv')['pid'].values\n",
    "\n",
    "# Define a DataFrame to hold the external authors\n",
    "external_authors = set()\n",
    "\n",
    "# Define an error counter\n",
    "error_cnt = 0\n",
    "\n",
    "# Iterate over the XML files\n",
    "for filename in tqdm(os.listdir('xml_files')):\n",
    "    if filename.endswith('.xml'):\n",
    "        # Parse the XML file\n",
    "        tree = ET.parse(os.path.join('xml_files', filename))\n",
    "        # Get the root element\n",
    "        root = tree.getroot()\n",
    "        # Iterate over the <r> elements\n",
    "        for r in root.findall('r'):\n",
    "            # Define a variable to store if a publication tag is found\n",
    "            found = False\n",
    "            # Get the publication element for every possible publication tag\n",
    "            for pt in publication_tags:\n",
    "                publication = r.find(pt)\n",
    "                if publication is not None:\n",
    "                    found = True\n",
    "                    # Extract the title, year, doi, and pids of all authors\n",
    "                    title = publication.find('title').text if publication.find('title') is not None else None\n",
    "                    if title is None:\n",
    "                        title = ET.tostring(publication.find('title'))\n",
    "                    key = publication.get('key') if publication.get('key') is not None else None\n",
    "                    year = publication.find('year').text if publication.find('year') is not None else None\n",
    "                    authors = []\n",
    "                    e_authors = []\n",
    "                    for a_tag in author_tags:\n",
    "                        # Add the authors for the current publication to the list\n",
    "                        authors.extend([author.get('pid').replace('/', '-') for author in publication.findall(a_tag) if author.get('pid').replace('/', '-') in our_authors])\n",
    "                        # Add the external authors for the current publication to the list\n",
    "                        e_authors.extend([author.get('pid').replace('/', '-') for author in publication.findall(a_tag) if author.get('pid').replace('/', '-') not in our_authors])\n",
    "                        # Add the external authors for the current publication to the set\n",
    "                        external_authors.update(e_authors)\n",
    "                    # Check if the title, year or key\n",
    "                    if title is not None and year is not None and key is not None and len(authors) > 0:\n",
    "                        # Append the extracted data to the list\n",
    "                        papers.append([title, year, key, authors, e_authors, pt, filename])\n",
    "                    else:\n",
    "                        print(f'Error: {filename}, {key} --> title: {title}, year: {year}, key: {key}, authors: {authors}')\n",
    "                        error_cnt += 1\n",
    "            if not found:\n",
    "                print(f'Error: {filename}, {r}')\n",
    "                error_cnt += 1\n",
    "                    \n",
    "# Display if there are any errors\n",
    "if error_cnt == 0:\n",
    "    print('No errors occurred')\n",
    "else:\n",
    "    print(f'{error_cnt} errors occurred')\n",
    "\n",
    "# Convert the list to a pandas DataFrame\n",
    "papers_df = pd.DataFrame(papers, columns=['Title', 'Year', 'Key', 'Authors', 'External Authors', 'Publication Type' ,'file'])\n",
    "# Convert lists in 'Authors' and 'External Authors' columns to tuples\n",
    "papers_df['Authors'] = papers_df['Authors'].apply(str)\n",
    "papers_df['External Authors'] = papers_df['External Authors'].apply(str)\n",
    "# Group by all columns except 'file' and aggregate the values\n",
    "papers_df = papers_df.groupby(['Title', 'Year', 'Key', 'Authors', 'External Authors', 'Publication Type']).agg(lambda x: set(x)).reset_index()\n",
    "# Convert the 'files' set to string\n",
    "papers_df['file'] = papers_df['file'].apply(str)\n",
    "# Remove the duplicates\n",
    "papers_df = papers_df.drop_duplicates()\n",
    "# Save the DataFrame to a CSV file\n",
    "papers_df.to_csv('papers.csv', index=False)\n",
    "\n",
    "# Convert the set to a DataFrame\n",
    "external_authors_df = pd.DataFrame(list(external_authors), columns=['pid'])\n",
    "# Save the DataFrame to a CSV file\n",
    "external_authors_df.to_csv('external_authors.csv', index=False)"
   ],
   "id": "91cd5bb88a415945",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1052/1052 [00:16<00:00, 64.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors occurred\n"
     ]
    }
   ],
   "execution_count": 204
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We have successfully converted the XML files to a DataFrame. We have the information about the publications of the data scientists: title, year, key, authors, and external authors. We have also created a DataFrame for the external authors (not given in the initial data). We can now proceed to the data analysis.",
   "id": "1e7edfa501946aab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.6 Data Analysis & Cleaning\n",
    "In this section, we will perform simple data analysis on the collected data."
   ],
   "id": "1e9f81786a49882b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T09:30:15.447739Z",
     "start_time": "2024-04-07T09:30:15.128148Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Read the CSV files into DataFrames\n",
    "papers_df = pd.read_csv('papers.csv')\n",
    "\n",
    "# Show the size of the DataFrame\n",
    "print(f'The DataFrame has {papers_df.shape[0]} rows and {papers_df.shape[1]} columns')\n",
    "\n",
    "# Print the number of duplicated values\n",
    "print(f'\\n The number of duplicated values is {papers_df.duplicated().sum()}')\n",
    "\n",
    "# Display the number of duplicated values for each column\n",
    "print('\\nNumber of duplicated values for each column:')\n",
    "for column in ['Title', 'Key']:\n",
    "    print(f'-The column \"{column}\" has {papers_df.duplicated(subset=column, keep=False).sum()} duplicated values')\n",
    "\n",
    "# Show some rows from the DataFrame with duplicated titles\n",
    "duplicated_titles = papers_df[papers_df.duplicated(subset='Title', keep=False)]\n",
    "duplicated_titles"
   ],
   "id": "967255bf640bdeda",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DataFrame has 102549 rows and 7 columns\n",
      "\n",
      " The number of duplicated values is 0\n",
      "\n",
      "Number of duplicated values for each column:\n",
      "-The column \"Title\" has 15431 duplicated values\n",
      "-The column \"Key\" has 2 duplicated values\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                                    Title  Year  \\\n",
       "66                b'<title><i>k</i>-Anonymity.</title>\\n'  2007   \n",
       "67                b'<title><i>k</i>-Anonymity.</title>\\n'  2018   \n",
       "81      \"A Virus Has No Religion\": Analyzing Islamopho...  2021   \n",
       "82      \"A Virus Has No Religion\": Analyzing Islamopho...  2021   \n",
       "95      \"Diversity and Uncertainty in Moderation\" are ...  2022   \n",
       "...                                                   ...   ...   \n",
       "102464                                         Éditorial.  2008   \n",
       "102465                                         Éditorial.  2009   \n",
       "102466                                         Éditorial.  2012   \n",
       "102467                                         Éditorial.  2014   \n",
       "102468                                         Éditorial.  2017   \n",
       "\n",
       "                                   Key               Authors  \\\n",
       "66            series/ais/CirianiVFS07a       ['s-PSamarati']   \n",
       "67      reference/db/Domingo-Ferrer18d  ['d-JDomingoFerrer']   \n",
       "81              conf/ht/ChandraRSGBK21           ['97-5147']   \n",
       "82        journals/corr/abs-2107-05104           ['97-5147']   \n",
       "95                conf/naacl/KumarDC22           ['29-5841']   \n",
       "...                                ...                   ...   \n",
       "102464      journals/isi/BoucelmaHLP08         ['p-JMPetit']   \n",
       "102465        journals/isi/ServigneZ09  ['z-KarineZeitouni']   \n",
       "102466         journals/isi/Bouganim12     ['b-LucBouganim']   \n",
       "102467            journals/isi/Petit14         ['p-JMPetit']   \n",
       "102468          journals/isi/Toumani17   ['t-FaroukToumani']   \n",
       "\n",
       "                                         External Authors Publication Type  \\\n",
       "66      ['c-ValentinaCiriani', 'v-SabrinaDeCapitanidiV...     incollection   \n",
       "67                                                     []     incollection   \n",
       "81      ['152-3504', '283-5340', '283-5610', '06-5843-...    inproceedings   \n",
       "82      ['152-3504', '283-5340', '283-5610', '06-5843-...          article   \n",
       "95                                ['234-8570', '71-4474']    inproceedings   \n",
       "...                                                   ...              ...   \n",
       "102464  ['b-OmarBoucelma', 'h-MohandSaidHacid', 'l-The...          article   \n",
       "102465                                        ['49-5791']          article   \n",
       "102466                                                 []          article   \n",
       "102467                                                 []          article   \n",
       "102468                                                 []          article   \n",
       "\n",
       "                            file  \n",
       "66           {'s-PSamarati.xml'}  \n",
       "67      {'d-JDomingoFerrer.xml'}  \n",
       "81               {'97-5147.xml'}  \n",
       "82               {'97-5147.xml'}  \n",
       "95               {'29-5841.xml'}  \n",
       "...                          ...  \n",
       "102464         {'p-JMPetit.xml'}  \n",
       "102465  {'z-KarineZeitouni.xml'}  \n",
       "102466     {'b-LucBouganim.xml'}  \n",
       "102467         {'p-JMPetit.xml'}  \n",
       "102468   {'t-FaroukToumani.xml'}  \n",
       "\n",
       "[15431 rows x 7 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Year</th>\n",
       "      <th>Key</th>\n",
       "      <th>Authors</th>\n",
       "      <th>External Authors</th>\n",
       "      <th>Publication Type</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>b'&lt;title&gt;&lt;i&gt;k&lt;/i&gt;-Anonymity.&lt;/title&gt;\\n'</td>\n",
       "      <td>2007</td>\n",
       "      <td>series/ais/CirianiVFS07a</td>\n",
       "      <td>['s-PSamarati']</td>\n",
       "      <td>['c-ValentinaCiriani', 'v-SabrinaDeCapitanidiV...</td>\n",
       "      <td>incollection</td>\n",
       "      <td>{'s-PSamarati.xml'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>b'&lt;title&gt;&lt;i&gt;k&lt;/i&gt;-Anonymity.&lt;/title&gt;\\n'</td>\n",
       "      <td>2018</td>\n",
       "      <td>reference/db/Domingo-Ferrer18d</td>\n",
       "      <td>['d-JDomingoFerrer']</td>\n",
       "      <td>[]</td>\n",
       "      <td>incollection</td>\n",
       "      <td>{'d-JDomingoFerrer.xml'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>\"A Virus Has No Religion\": Analyzing Islamopho...</td>\n",
       "      <td>2021</td>\n",
       "      <td>conf/ht/ChandraRSGBK21</td>\n",
       "      <td>['97-5147']</td>\n",
       "      <td>['152-3504', '283-5340', '283-5610', '06-5843-...</td>\n",
       "      <td>inproceedings</td>\n",
       "      <td>{'97-5147.xml'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>\"A Virus Has No Religion\": Analyzing Islamopho...</td>\n",
       "      <td>2021</td>\n",
       "      <td>journals/corr/abs-2107-05104</td>\n",
       "      <td>['97-5147']</td>\n",
       "      <td>['152-3504', '283-5340', '283-5610', '06-5843-...</td>\n",
       "      <td>article</td>\n",
       "      <td>{'97-5147.xml'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>\"Diversity and Uncertainty in Moderation\" are ...</td>\n",
       "      <td>2022</td>\n",
       "      <td>conf/naacl/KumarDC22</td>\n",
       "      <td>['29-5841']</td>\n",
       "      <td>['234-8570', '71-4474']</td>\n",
       "      <td>inproceedings</td>\n",
       "      <td>{'29-5841.xml'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102464</th>\n",
       "      <td>Éditorial.</td>\n",
       "      <td>2008</td>\n",
       "      <td>journals/isi/BoucelmaHLP08</td>\n",
       "      <td>['p-JMPetit']</td>\n",
       "      <td>['b-OmarBoucelma', 'h-MohandSaidHacid', 'l-The...</td>\n",
       "      <td>article</td>\n",
       "      <td>{'p-JMPetit.xml'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102465</th>\n",
       "      <td>Éditorial.</td>\n",
       "      <td>2009</td>\n",
       "      <td>journals/isi/ServigneZ09</td>\n",
       "      <td>['z-KarineZeitouni']</td>\n",
       "      <td>['49-5791']</td>\n",
       "      <td>article</td>\n",
       "      <td>{'z-KarineZeitouni.xml'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102466</th>\n",
       "      <td>Éditorial.</td>\n",
       "      <td>2012</td>\n",
       "      <td>journals/isi/Bouganim12</td>\n",
       "      <td>['b-LucBouganim']</td>\n",
       "      <td>[]</td>\n",
       "      <td>article</td>\n",
       "      <td>{'b-LucBouganim.xml'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102467</th>\n",
       "      <td>Éditorial.</td>\n",
       "      <td>2014</td>\n",
       "      <td>journals/isi/Petit14</td>\n",
       "      <td>['p-JMPetit']</td>\n",
       "      <td>[]</td>\n",
       "      <td>article</td>\n",
       "      <td>{'p-JMPetit.xml'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102468</th>\n",
       "      <td>Éditorial.</td>\n",
       "      <td>2017</td>\n",
       "      <td>journals/isi/Toumani17</td>\n",
       "      <td>['t-FaroukToumani']</td>\n",
       "      <td>[]</td>\n",
       "      <td>article</td>\n",
       "      <td>{'t-FaroukToumani.xml'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15431 rows × 7 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 239
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As we can see, we have some duplicated values again. This time, the duplicated values are caused by the same publication being assigned to multiple conferences, journals, etc. As a result, we have publications with same title, year, authors, etc. but different keys. We will remove the duplicated values based on the 'Title' and 'Authors' columns.",
   "id": "cf833098016a599c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T09:30:15.507389Z",
     "start_time": "2024-04-07T09:30:15.448757Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Remove the duplicated values based on the 'Title', 'Year', 'Authors', and 'External Authors' columns\n",
    "papers_df = papers_df.drop_duplicates(subset=['Title', 'Authors'])\n",
    "\n",
    "# Show the size of the DataFrame\n",
    "print(f'The DataFrame has {papers_df.shape[0]} rows and {papers_df.shape[1]} columns')\n",
    "\n",
    "# Display the number of duplicated values for each column\n",
    "print('\\nNumber of duplicated values for each column:')\n",
    "for column in ['Title', 'Key']:\n",
    "    print(f'-The column \"{column}\" has {papers_df.duplicated(subset=column, keep=False).sum()} duplicated values')\n",
    "\n",
    "# Show some rows from the DataFrame with duplicated titles\n",
    "duplicated_titles = papers_df[papers_df.duplicated(subset='Title', keep=False)]\n",
    "duplicated_titles"
   ],
   "id": "30c7351a0a989da6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DataFrame has 94942 rows and 7 columns\n",
      "\n",
      "Number of duplicated values for each column:\n",
      "-The column \"Title\" has 605 duplicated values\n",
      "-The column \"Key\" has 0 duplicated values\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                          Title  Year  \\\n",
       "66      b'<title><i>k</i>-Anonymity.</title>\\n'  2007   \n",
       "67      b'<title><i>k</i>-Anonymity.</title>\\n'  2018   \n",
       "227                                           (  2007   \n",
       "228                                           (  2009   \n",
       "300                                          10  2007   \n",
       "...                                         ...   ...   \n",
       "102463                               Éditorial.  2006   \n",
       "102464                               Éditorial.  2008   \n",
       "102465                               Éditorial.  2009   \n",
       "102466                               Éditorial.  2012   \n",
       "102468                               Éditorial.  2017   \n",
       "\n",
       "                                   Key  \\\n",
       "66            series/ais/CirianiVFS07a   \n",
       "67      reference/db/Domingo-Ferrer18d   \n",
       "227              conf/waim/WongLYHFP07   \n",
       "228            journals/jiis/WongLFW09   \n",
       "300               conf/icde/AntovaKO07   \n",
       "...                                ...   \n",
       "102463            journals/isi/Mothe06   \n",
       "102464      journals/isi/BoucelmaHLP08   \n",
       "102465        journals/isi/ServigneZ09   \n",
       "102466         journals/isi/Bouganim12   \n",
       "102468          journals/isi/Toumani17   \n",
       "\n",
       "                                        Authors  \\\n",
       "66                              ['s-PSamarati']   \n",
       "67                         ['d-JDomingoFerrer']   \n",
       "227       ['w-RaymondChiWingWong', 'p-JianPei']   \n",
       "228        ['w-RaymondChiWingWong', 'w-KeWang']   \n",
       "300     ['a-LyublenaAntova', 'k-ChristophKoch']   \n",
       "...                                         ...   \n",
       "102463                       ['m-JosianeMothe']   \n",
       "102464                            ['p-JMPetit']   \n",
       "102465                     ['z-KarineZeitouni']   \n",
       "102466                        ['b-LucBouganim']   \n",
       "102468                      ['t-FaroukToumani']   \n",
       "\n",
       "                                         External Authors Publication Type  \\\n",
       "66      ['c-ValentinaCiriani', 'v-SabrinaDeCapitanidiV...     incollection   \n",
       "67                                                     []     incollection   \n",
       "227     ['30-2791', '95-578-1', '13-4665', 'f-AdaWaiCh...    inproceedings   \n",
       "228                         ['20-1583', 'f-AdaWaiCheeFu']          article   \n",
       "300                                      ['o-DanOlteanu']    inproceedings   \n",
       "...                                                   ...              ...   \n",
       "102463                                                 []          article   \n",
       "102464  ['b-OmarBoucelma', 'h-MohandSaidHacid', 'l-The...          article   \n",
       "102465                                        ['49-5791']          article   \n",
       "102466                                                 []          article   \n",
       "102468                                                 []          article   \n",
       "\n",
       "                                                   file  \n",
       "66                                  {'s-PSamarati.xml'}  \n",
       "67                             {'d-JDomingoFerrer.xml'}  \n",
       "227       {'p-JianPei.xml', 'w-RaymondChiWingWong.xml'}  \n",
       "228        {'w-KeWang.xml', 'w-RaymondChiWingWong.xml'}  \n",
       "300     {'k-ChristophKoch.xml', 'a-LyublenaAntova.xml'}  \n",
       "...                                                 ...  \n",
       "102463                           {'m-JosianeMothe.xml'}  \n",
       "102464                                {'p-JMPetit.xml'}  \n",
       "102465                         {'z-KarineZeitouni.xml'}  \n",
       "102466                            {'b-LucBouganim.xml'}  \n",
       "102468                          {'t-FaroukToumani.xml'}  \n",
       "\n",
       "[605 rows x 7 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Year</th>\n",
       "      <th>Key</th>\n",
       "      <th>Authors</th>\n",
       "      <th>External Authors</th>\n",
       "      <th>Publication Type</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>b'&lt;title&gt;&lt;i&gt;k&lt;/i&gt;-Anonymity.&lt;/title&gt;\\n'</td>\n",
       "      <td>2007</td>\n",
       "      <td>series/ais/CirianiVFS07a</td>\n",
       "      <td>['s-PSamarati']</td>\n",
       "      <td>['c-ValentinaCiriani', 'v-SabrinaDeCapitanidiV...</td>\n",
       "      <td>incollection</td>\n",
       "      <td>{'s-PSamarati.xml'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>b'&lt;title&gt;&lt;i&gt;k&lt;/i&gt;-Anonymity.&lt;/title&gt;\\n'</td>\n",
       "      <td>2018</td>\n",
       "      <td>reference/db/Domingo-Ferrer18d</td>\n",
       "      <td>['d-JDomingoFerrer']</td>\n",
       "      <td>[]</td>\n",
       "      <td>incollection</td>\n",
       "      <td>{'d-JDomingoFerrer.xml'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>(</td>\n",
       "      <td>2007</td>\n",
       "      <td>conf/waim/WongLYHFP07</td>\n",
       "      <td>['w-RaymondChiWingWong', 'p-JianPei']</td>\n",
       "      <td>['30-2791', '95-578-1', '13-4665', 'f-AdaWaiCh...</td>\n",
       "      <td>inproceedings</td>\n",
       "      <td>{'p-JianPei.xml', 'w-RaymondChiWingWong.xml'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>(</td>\n",
       "      <td>2009</td>\n",
       "      <td>journals/jiis/WongLFW09</td>\n",
       "      <td>['w-RaymondChiWingWong', 'w-KeWang']</td>\n",
       "      <td>['20-1583', 'f-AdaWaiCheeFu']</td>\n",
       "      <td>article</td>\n",
       "      <td>{'w-KeWang.xml', 'w-RaymondChiWingWong.xml'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>10</td>\n",
       "      <td>2007</td>\n",
       "      <td>conf/icde/AntovaKO07</td>\n",
       "      <td>['a-LyublenaAntova', 'k-ChristophKoch']</td>\n",
       "      <td>['o-DanOlteanu']</td>\n",
       "      <td>inproceedings</td>\n",
       "      <td>{'k-ChristophKoch.xml', 'a-LyublenaAntova.xml'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102463</th>\n",
       "      <td>Éditorial.</td>\n",
       "      <td>2006</td>\n",
       "      <td>journals/isi/Mothe06</td>\n",
       "      <td>['m-JosianeMothe']</td>\n",
       "      <td>[]</td>\n",
       "      <td>article</td>\n",
       "      <td>{'m-JosianeMothe.xml'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102464</th>\n",
       "      <td>Éditorial.</td>\n",
       "      <td>2008</td>\n",
       "      <td>journals/isi/BoucelmaHLP08</td>\n",
       "      <td>['p-JMPetit']</td>\n",
       "      <td>['b-OmarBoucelma', 'h-MohandSaidHacid', 'l-The...</td>\n",
       "      <td>article</td>\n",
       "      <td>{'p-JMPetit.xml'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102465</th>\n",
       "      <td>Éditorial.</td>\n",
       "      <td>2009</td>\n",
       "      <td>journals/isi/ServigneZ09</td>\n",
       "      <td>['z-KarineZeitouni']</td>\n",
       "      <td>['49-5791']</td>\n",
       "      <td>article</td>\n",
       "      <td>{'z-KarineZeitouni.xml'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102466</th>\n",
       "      <td>Éditorial.</td>\n",
       "      <td>2012</td>\n",
       "      <td>journals/isi/Bouganim12</td>\n",
       "      <td>['b-LucBouganim']</td>\n",
       "      <td>[]</td>\n",
       "      <td>article</td>\n",
       "      <td>{'b-LucBouganim.xml'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102468</th>\n",
       "      <td>Éditorial.</td>\n",
       "      <td>2017</td>\n",
       "      <td>journals/isi/Toumani17</td>\n",
       "      <td>['t-FaroukToumani']</td>\n",
       "      <td>[]</td>\n",
       "      <td>article</td>\n",
       "      <td>{'t-FaroukToumani.xml'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>605 rows × 7 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 240
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "After removing the duplicated publications, we can see that there are some very strange publications with some characters are titles. We will remove them as well as they are not useful for the analysis and may be some errors in the data collection. We will remove all publications with less than 2 words in the title.\n",
    "\n",
    "We can also see that there are many publications with only 1 author. We will remove them as well as they are not useful for the analysis, they are not collaborations and cannot be used for the network analysis."
   ],
   "id": "13db505af805a939"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T09:30:17.375745Z",
     "start_time": "2024-04-07T09:30:15.972185Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Remove the publications with less than 2 words in the title\n",
    "papers_df = papers_df[papers_df['Title'].apply(lambda x: len(x.split()) >= 2)]\n",
    "\n",
    "# Remove the publications with sum of authors and external authors less than 2\n",
    "papers_df = papers_df[papers_df.apply(lambda r: len(ast.literal_eval(r['Authors'])) + len(ast.literal_eval(r['External Authors'])) >= 2, axis=1)]\n",
    "\n",
    "# Show the size of the DataFrame\n",
    "print(f'The DataFrame has {papers_df.shape[0]} rows and {papers_df.shape[1]} columns')\n",
    "\n",
    "# Display the number of duplicated values for each column\n",
    "print('\\nNumber of duplicated values for each column:')\n",
    "for column in ['Title', 'Key']:\n",
    "    print(f'-The column \"{column}\" has {papers_df.duplicated(subset=column, keep=False).sum()} duplicated values')\n",
    "\n",
    "# Show some rows from the DataFrame with duplicated titles\n",
    "duplicated_titles = papers_df[papers_df.duplicated(subset='Title', keep=False)]\n",
    "duplicated_titles"
   ],
   "id": "453c96ede114d49b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DataFrame has 91001 rows and 7 columns\n",
      "\n",
      "Number of duplicated values for each column:\n",
      "-The column \"Title\" has 270 duplicated values\n",
      "-The column \"Key\" has 0 duplicated values\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                                    Title  Year  \\\n",
       "1373    A Decentralized Approach for Controlled Sharin...  2003   \n",
       "1374    A Decentralized Approach for Controlled Sharin...  2006   \n",
       "3826                            A Query Language for XML.  1998   \n",
       "3827                            A Query Language for XML.  1999   \n",
       "3859    A Queueing-Theoretic Framework for Vehicle Dis...  2019   \n",
       "...                                                   ...   ...   \n",
       "100122                                      Web Services.  2004   \n",
       "100913  WiSer: A Highly Available HTAP DBMS for IoT Ap...  2019   \n",
       "100914  WiSer: A Highly Available HTAP DBMS for IoT Ap...  2019   \n",
       "101203                      Workshop Organizers' Message.  2009   \n",
       "101204                      Workshop Organizers' Message.  2009   \n",
       "\n",
       "                                       Key  \\\n",
       "1373                conf/dbsec/BertinoFS03   \n",
       "1374                 conf/cscwd/BertinoS06   \n",
       "3826                 conf/w3c/FernandezS98   \n",
       "3827             journals/cn/DeutschFFLS99   \n",
       "3859                   conf/icde/0003FCW19   \n",
       "...                                    ...   \n",
       "100122           journals/insk/KossmannL04   \n",
       "100913  conf/bigdataconf/BarberSTTWGGL0M19   \n",
       "100914        journals/corr/abs-1908-01908   \n",
       "101203          conf/dasfaa/SadiqDZYADLX09   \n",
       "101204                 conf/dasfaa/WongF09   \n",
       "\n",
       "                                                  Authors  \\\n",
       "1373    ['b-ElisaBertino', 'f-ElenaFerrari', 's-AnnaCi...   \n",
       "1374       ['b-ElisaBertino', 's-AnnaCinziaSquicciarini']   \n",
       "3826                                       ['s-DanSuciu']   \n",
       "3827     ['d-AlinDeutsch', 'h-AlonYHalevy', 's-DanSuciu']   \n",
       "3859                        ['76-185-3', 'c-LeiChen0002']   \n",
       "...                                                   ...   \n",
       "100122                               ['k-DonaldKossmann']   \n",
       "100913  ['27-3375', '86-9726', '07-11533', 'm-CMohan',...   \n",
       "100914  ['m-CMohan', 'm-ReneMuller', 'r-VijayshankarRa...   \n",
       "101203                     ['z-XiaofangZhou', 'a-WGAref']   \n",
       "101204                           ['w-RaymondChiWingWong']   \n",
       "\n",
       "                                         External Authors Publication Type  \\\n",
       "1373                                                   []    inproceedings   \n",
       "1374                                                   []    inproceedings   \n",
       "3826                                 ['f-MaryFFernandez']    inproceedings   \n",
       "3827                      ['f-MaryFFernandez', '70-3007']          article   \n",
       "3859                               ['97-164', '181-2834']    inproceedings   \n",
       "...                                                   ...              ...   \n",
       "100122                                 ['l-FrankLeymann']          article   \n",
       "100913  ['33-3527', '16-5412', '96-122', '192-4573', '...    inproceedings   \n",
       "100914  ['33-3527', '96-122', '192-4573', '21-2457', '...          article   \n",
       "101203  ['s-SWSadiq', '97-1199', '86-2859-1', 'd-AlexD...    inproceedings   \n",
       "101204                                 ['f-AdaWaiCheeFu']    inproceedings   \n",
       "\n",
       "                                                     file  \n",
       "1373    {'f-ElenaFerrari.xml', 'b-ElisaBertino.xml', '...  \n",
       "1374    {'b-ElisaBertino.xml', 's-AnnaCinziaSquicciari...  \n",
       "3826                                   {'s-DanSuciu.xml'}  \n",
       "3827    {'h-AlonYHalevy.xml', 's-DanSuciu.xml', 'd-Ali...  \n",
       "3859                {'c-LeiChen0002.xml', '76-185-3.xml'}  \n",
       "...                                                   ...  \n",
       "100122                           {'k-DonaldKossmann.xml'}  \n",
       "100913  {'27-3375.xml', 'm-ReneMuller.xml', '07-11533....  \n",
       "100914  {'27-3375.xml', 'm-ReneMuller.xml', '07-11533....  \n",
       "101203             {'a-WGAref.xml', 'z-XiaofangZhou.xml'}  \n",
       "101204                       {'w-RaymondChiWingWong.xml'}  \n",
       "\n",
       "[270 rows x 7 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Year</th>\n",
       "      <th>Key</th>\n",
       "      <th>Authors</th>\n",
       "      <th>External Authors</th>\n",
       "      <th>Publication Type</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1373</th>\n",
       "      <td>A Decentralized Approach for Controlled Sharin...</td>\n",
       "      <td>2003</td>\n",
       "      <td>conf/dbsec/BertinoFS03</td>\n",
       "      <td>['b-ElisaBertino', 'f-ElenaFerrari', 's-AnnaCi...</td>\n",
       "      <td>[]</td>\n",
       "      <td>inproceedings</td>\n",
       "      <td>{'f-ElenaFerrari.xml', 'b-ElisaBertino.xml', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374</th>\n",
       "      <td>A Decentralized Approach for Controlled Sharin...</td>\n",
       "      <td>2006</td>\n",
       "      <td>conf/cscwd/BertinoS06</td>\n",
       "      <td>['b-ElisaBertino', 's-AnnaCinziaSquicciarini']</td>\n",
       "      <td>[]</td>\n",
       "      <td>inproceedings</td>\n",
       "      <td>{'b-ElisaBertino.xml', 's-AnnaCinziaSquicciari...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3826</th>\n",
       "      <td>A Query Language for XML.</td>\n",
       "      <td>1998</td>\n",
       "      <td>conf/w3c/FernandezS98</td>\n",
       "      <td>['s-DanSuciu']</td>\n",
       "      <td>['f-MaryFFernandez']</td>\n",
       "      <td>inproceedings</td>\n",
       "      <td>{'s-DanSuciu.xml'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3827</th>\n",
       "      <td>A Query Language for XML.</td>\n",
       "      <td>1999</td>\n",
       "      <td>journals/cn/DeutschFFLS99</td>\n",
       "      <td>['d-AlinDeutsch', 'h-AlonYHalevy', 's-DanSuciu']</td>\n",
       "      <td>['f-MaryFFernandez', '70-3007']</td>\n",
       "      <td>article</td>\n",
       "      <td>{'h-AlonYHalevy.xml', 's-DanSuciu.xml', 'd-Ali...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3859</th>\n",
       "      <td>A Queueing-Theoretic Framework for Vehicle Dis...</td>\n",
       "      <td>2019</td>\n",
       "      <td>conf/icde/0003FCW19</td>\n",
       "      <td>['76-185-3', 'c-LeiChen0002']</td>\n",
       "      <td>['97-164', '181-2834']</td>\n",
       "      <td>inproceedings</td>\n",
       "      <td>{'c-LeiChen0002.xml', '76-185-3.xml'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100122</th>\n",
       "      <td>Web Services.</td>\n",
       "      <td>2004</td>\n",
       "      <td>journals/insk/KossmannL04</td>\n",
       "      <td>['k-DonaldKossmann']</td>\n",
       "      <td>['l-FrankLeymann']</td>\n",
       "      <td>article</td>\n",
       "      <td>{'k-DonaldKossmann.xml'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100913</th>\n",
       "      <td>WiSer: A Highly Available HTAP DBMS for IoT Ap...</td>\n",
       "      <td>2019</td>\n",
       "      <td>conf/bigdataconf/BarberSTTWGGL0M19</td>\n",
       "      <td>['27-3375', '86-9726', '07-11533', 'm-CMohan',...</td>\n",
       "      <td>['33-3527', '16-5412', '96-122', '192-4573', '...</td>\n",
       "      <td>inproceedings</td>\n",
       "      <td>{'27-3375.xml', 'm-ReneMuller.xml', '07-11533....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100914</th>\n",
       "      <td>WiSer: A Highly Available HTAP DBMS for IoT Ap...</td>\n",
       "      <td>2019</td>\n",
       "      <td>journals/corr/abs-1908-01908</td>\n",
       "      <td>['m-CMohan', 'm-ReneMuller', 'r-VijayshankarRa...</td>\n",
       "      <td>['33-3527', '96-122', '192-4573', '21-2457', '...</td>\n",
       "      <td>article</td>\n",
       "      <td>{'27-3375.xml', 'm-ReneMuller.xml', '07-11533....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101203</th>\n",
       "      <td>Workshop Organizers' Message.</td>\n",
       "      <td>2009</td>\n",
       "      <td>conf/dasfaa/SadiqDZYADLX09</td>\n",
       "      <td>['z-XiaofangZhou', 'a-WGAref']</td>\n",
       "      <td>['s-SWSadiq', '97-1199', '86-2859-1', 'd-AlexD...</td>\n",
       "      <td>inproceedings</td>\n",
       "      <td>{'a-WGAref.xml', 'z-XiaofangZhou.xml'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101204</th>\n",
       "      <td>Workshop Organizers' Message.</td>\n",
       "      <td>2009</td>\n",
       "      <td>conf/dasfaa/WongF09</td>\n",
       "      <td>['w-RaymondChiWingWong']</td>\n",
       "      <td>['f-AdaWaiCheeFu']</td>\n",
       "      <td>inproceedings</td>\n",
       "      <td>{'w-RaymondChiWingWong.xml'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>270 rows × 7 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 241
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Still, we have some more duplicated values. After further inspection we can see that the duplicated values are caused by the same publication being assigned to multiple conferences, journals, etc. but with different keys. Also, we can see that the duplicated publications' authors are subsets of each other. For example, the publication with authors A, B, C is duplicated with authors A, B. We will combine the duplicated publications and keep the earliest year and the authors that are the superset of the authors of the duplicated publications.",
   "id": "58cdf58225a9b91f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T09:31:00.478690Z",
     "start_time": "2024-04-07T09:31:00.437711Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define a list to hold the combined publications to be appended to the DataFrame later\n",
    "combined_pubs = []\n",
    "\n",
    "# For each unique title in the duplicated titles, combine the duplicated publications' authors and keep the earliest year\n",
    "for title in duplicated_titles['Title'].unique():\n",
    "    # Get the duplicated publications with the same title\n",
    "    duplicated_pubs = papers_df[papers_df['Title'] == title]\n",
    "    # Get the earliest year\n",
    "    earliest_year = duplicated_pubs['Year'].min()\n",
    "    # Get the authors of the duplicated publications\n",
    "    authors = set()\n",
    "    e_authors = set()\n",
    "    # Get the publication types of the duplicated publications\n",
    "    p_types = set()\n",
    "    # Choose a random key\n",
    "    key = duplicated_pubs['Key'].iloc[0]\n",
    "    # Choose a random file\n",
    "    file = duplicated_pubs['file'].iloc[0]\n",
    "    for index, row in duplicated_pubs.iterrows():\n",
    "        authors.update(ast.literal_eval(row['Authors']))\n",
    "        e_authors.update(ast.literal_eval(row['External Authors']))\n",
    "        p_types.add(row['Publication Type'])\n",
    "    if len(p_types) > 1:\n",
    "        p_types = str(p_types)\n",
    "    else:\n",
    "        p_types = p_types.pop()\n",
    "    # Remove the duplicated publications\n",
    "    papers_df = papers_df[~papers_df.index.isin(duplicated_pubs.index)]\n",
    "    # Append the combined publication\n",
    "    combined_pubs.append([title, earliest_year, key, authors, e_authors, p_types, file])\n",
    "\n",
    "# Append the combined publications to the DataFrame\n",
    "papers_df = pd.concat([papers_df, pd.DataFrame(combined_pubs, columns=papers_df.columns)]).reset_index(drop=True)\n",
    "\n",
    "# Show the size of the DataFrame\n",
    "print(f'The DataFrame has {papers_df.shape[0]} rows and {papers_df.shape[1]} columns')\n",
    "\n",
    "# Display the number of duplicated values for each column\n",
    "print('\\nNumber of duplicated values for each column:')\n",
    "for column in ['Title', 'Key']:\n",
    "    print(f'-The column \"{column}\" has {papers_df.duplicated(subset=column, keep=False).sum()} duplicated values')\n",
    "\n",
    "# Show some rows from the DataFrame with duplicated titles\n",
    "duplicated_titles = papers_df[papers_df.duplicated(subset='Title', keep=False)]\n",
    "duplicated_titles"
   ],
   "id": "b202af62fae89c77",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DataFrame has 90837 rows and 7 columns\n",
      "\n",
      "Number of duplicated values for each column:\n",
      "-The column \"Title\" has 0 duplicated values\n",
      "-The column \"Key\" has 0 duplicated values\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Title, Year, Key, Authors, External Authors, Publication Type, file]\n",
       "Index: []"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Year</th>\n",
       "      <th>Key</th>\n",
       "      <th>Authors</th>\n",
       "      <th>External Authors</th>\n",
       "      <th>Publication Type</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 243
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "After all the cleaning, we finally have a clean DataFrame with no duplicating values. We will now process to saving the DataFrame to a CSV file.",
   "id": "f0a53d04abb537aa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T09:32:26.501619Z",
     "start_time": "2024-04-07T09:32:26.268900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "papers_df.to_csv('papers_clean.csv', index=False)"
   ],
   "id": "1dd41902151de613",
   "outputs": [],
   "execution_count": 244
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, we will analyze the data in the cleaned DataFrame.",
   "id": "6bc9ee72986982db"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T09:38:04.078891Z",
     "start_time": "2024-04-07T09:38:03.622829Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "papers_df = pd.read_csv('papers_clean.csv')\n",
    "\n",
    "# Display the size of the DataFrame\n",
    "print(f'The DataFrame has {papers_df.shape[0]} rows and {papers_df.shape[1]} columns')\n",
    "\n",
    "# Display the number of removed rows from the initial DataFrame\n",
    "print(f'{pd.read_csv(\"papers.csv\").shape[0] - papers_df.shape[0]} rows were removed from the initial DataFrame')\n",
    "\n",
    "# Print the number of rows with duplicated titles\n",
    "print(f'The number of rows with duplicated titles is {papers_df[papers_df.duplicated(subset=\"Title\", keep=False)].shape[0]}')"
   ],
   "id": "ab7e618e75082b23",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DataFrame has 90837 rows and 7 columns\n",
      "11712 rows were removed from the initial DataFrame\n",
      "The number of rows with duplicated titles is 0\n"
     ]
    }
   ],
   "execution_count": 248
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We have successfully cleaned the data and removed the duplicated values. Now, we will proceed to the network construction and analysis.",
   "id": "94b3dcf60007a296"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Network Analysis",
   "id": "77bf770f27744298"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f22ee9b3fb57eb67"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
