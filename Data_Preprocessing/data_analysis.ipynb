{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Import the ElementTree module for easier XML parsing\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-10T04:10:37.608193Z",
     "start_time": "2024-03-10T04:10:37.600651Z"
    }
   },
   "id": "cfed6b44a5d8149",
   "execution_count": 118
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The CSV file already exists\n",
      "(1220, 5)\n"
     ]
    }
   ],
   "source": [
    "# Check if the Excel file is converted to a csv file\n",
    "if 'scientists_collab.csv' in os.listdir():\n",
    "    print('The CSV file already exists')\n",
    "else:\n",
    "    print('The Excel file has not been converted to a CSV file')\n",
    "    # Read the Excel file\n",
    "    df = pd.read_excel('DataScientists.xls')\n",
    "    # Convert the DataFrame to a CSV file\n",
    "    df.to_csv('scientists_collab.csv', index=False)\n",
    "    print('The Excel file has been converted to a CSV file')\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('scientists_collab.csv')\n",
    "\n",
    "# Print the size of the DataFrame\n",
    "print(df.shape)"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-10T03:26:05.197217Z",
     "start_time": "2024-03-10T03:26:05.192312Z"
    }
   },
   "id": "initial_id",
   "execution_count": 110
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1220/1220 [23:44<00:00,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://dblp.org/pid/39/1380.html', 'https://dblp.uni-trier.de/pers/c/Chakraborty:Anirban.html', 'https://dblp.org/pid/92/2769.html', 'https://dblp.org/pid/148/7268.html', 'https://dblp.uni-trier.de/pers/b/Barbosa:Denilson.html', 'https://dblp.org/pid/161/0102.html', 'https://dblp.org/pers/m/Mansour:Essam.html', 'https://dblp.org/pers/m/Mansour:Essam.html', 'https://dblp.org/pid/284/0968.html', 'https://dblp.org/pid/98/5721.html', 'https://dblp.org/pers/j/Jung:Hyungsoo.html', 'https://dblp.uni-trier.de/pers/p/Petrov:Ilia.html', 'https://dblp.uni-trier.de/pers/p/Petrov:Ilia.html', 'https://dblp.org/pers/h/Hui:Kai.html', 'https://dblp.org/pers/w/Weidlich:Matthias.html', 'https://dblp.uni-trier.de/pers/hd/n/Nikolic:Milos', 'https://dblp.dagstuhl.de/pid/y/TingYu.html', 'https://dblp.dagstuhl.de/pid/y/TingYu.html', 'https://dblp.uni-trier.de/pers/t/Tao:Yufei.html', 'https://dblp.uni-trier.de/pers/t/Tao:Yufei.html']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the list to hold the PIDs\n",
    "pids = []\n",
    "final_urls = []\n",
    "\n",
    "# Define a variable to monitor for errors\n",
    "errors = []\n",
    "\n",
    "# Iterate over the 'dblp' column\n",
    "for link in tqdm(df['dblp']):\n",
    "    while True:\n",
    "        # Send a GET request\n",
    "        response = requests.get(link)\n",
    "\n",
    "        # If the status code is 429 (Too Many Requests), wait for 60 seconds before trying again\n",
    "        if response.status_code == 429:\n",
    "            print('Too many requests, sleeping for 60 seconds...')\n",
    "            time.sleep(60)\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    # If the status code is not 200, append the link to the errors list\n",
    "    if response.status_code != 200:\n",
    "        errors.append(link)\n",
    "        pids.append('Error')\n",
    "        final_urls.append('Error')\n",
    "        continue\n",
    "        \n",
    "\n",
    "    # Get the final URL\n",
    "    final_url = response.url\n",
    "\n",
    "    # Define the regular expression pattern\n",
    "    pattern = r'pid/(.*).html'\n",
    "\n",
    "    # Find the PID in the URL\n",
    "    match = re.search(pattern, final_url)\n",
    "\n",
    "    if match:\n",
    "        # Extract the PID\n",
    "        pid = match.group(1)\n",
    "\n",
    "        # Replace '/' with '-'\n",
    "        pid = pid.replace('/', '-')\n",
    "\n",
    "        # Append the PID to the list\n",
    "        pids.append(pid)\n",
    "        final_urls.append(final_url)\n",
    "    else:\n",
    "        # Append the error to the PID list\n",
    "        pids.append('Error')\n",
    "        \n",
    "        # Append the error to the final URL list\n",
    "        final_urls.append('Error')\n",
    "        \n",
    "        # Append the error to the list\n",
    "        errors.append(link)\n",
    "        \n",
    "# Print the errors\n",
    "print(errors)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-10T03:49:49.523540Z",
     "start_time": "2024-03-10T03:26:05.198257Z"
    }
   },
   "id": "36a8b897d19f299a",
   "execution_count": 111
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following errors (20) were encountered:\n",
      "https://dblp.org/pid/39/1380.html\n",
      "https://dblp.uni-trier.de/pers/c/Chakraborty:Anirban.html\n",
      "https://dblp.org/pid/92/2769.html\n",
      "https://dblp.org/pid/148/7268.html\n",
      "https://dblp.uni-trier.de/pers/b/Barbosa:Denilson.html\n",
      "https://dblp.org/pid/161/0102.html\n",
      "https://dblp.org/pers/m/Mansour:Essam.html\n",
      "https://dblp.org/pers/m/Mansour:Essam.html\n",
      "https://dblp.org/pid/284/0968.html\n",
      "https://dblp.org/pid/98/5721.html\n",
      "https://dblp.org/pers/j/Jung:Hyungsoo.html\n",
      "https://dblp.uni-trier.de/pers/p/Petrov:Ilia.html\n",
      "https://dblp.uni-trier.de/pers/p/Petrov:Ilia.html\n",
      "https://dblp.org/pers/h/Hui:Kai.html\n",
      "https://dblp.org/pers/w/Weidlich:Matthias.html\n",
      "https://dblp.uni-trier.de/pers/hd/n/Nikolic:Milos\n",
      "https://dblp.dagstuhl.de/pid/y/TingYu.html\n",
      "https://dblp.dagstuhl.de/pid/y/TingYu.html\n",
      "https://dblp.uni-trier.de/pers/t/Tao:Yufei.html\n",
      "https://dblp.uni-trier.de/pers/t/Tao:Yufei.html\n"
     ]
    }
   ],
   "source": [
    "# If errors were encountered, print all the errors\n",
    "if len(errors) > 0:\n",
    "    print(f'The following errors ({len(errors)}) were encountered:')\n",
    "    for error in errors:\n",
    "        print(error)\n",
    "else:\n",
    "    print('No errors were encountered')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-10T03:49:49.528444Z",
     "start_time": "2024-03-10T03:49:49.525044Z"
    }
   },
   "id": "659d9778e210352f",
   "execution_count": 112
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error 410: Gone\n",
      "Error 410: Gone\n",
      "Error 404: Not Found\n",
      "Error 410: Gone\n",
      "Error 410: Gone\n",
      "Error 410: Gone\n",
      "Error 410: Gone\n",
      "Error 410: Gone\n",
      "Error 404: Not Found\n",
      "Error 410: Gone\n",
      "Error 410: Gone\n",
      "Error 410: Gone\n",
      "Error 410: Gone\n",
      "Error 410: Gone\n",
      "Error 410: Gone\n",
      "Error 410: Gone\n",
      "Error 410: Gone\n",
      "Error 410: Gone\n",
      "Error 410: Gone\n",
      "Error 410: Gone\n"
     ]
    }
   ],
   "source": [
    "# For each error link, print the h1 tag content\n",
    "for error in errors:\n",
    "    # Send a GET request\n",
    "    response = requests.get(error)\n",
    "\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find the h1 tag\n",
    "    h1 = soup.find('h1')\n",
    "\n",
    "    # Print the h1 tag content\n",
    "    print(h1.text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-10T03:50:06.045691Z",
     "start_time": "2024-03-10T03:49:49.530966Z"
    }
   },
   "id": "344d41d91e852124",
   "execution_count": 113
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 rows were removed because their links were broken, led to Error 410\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('scientists_collab.csv')\n",
    "# Define the initial number of rows\n",
    "initial_rows = df.shape[0]\n",
    "\n",
    "# Add the PIDs and Final URLs to the DataFrame\n",
    "df['pid'] = pids\n",
    "df['final_url'] = final_urls\n",
    "\n",
    "# Remove the rows where the PID is 'Error' OR the final URL is 'Error'\n",
    "df = df[(df['pid'] != 'Error') & (df['final_url'] != 'Error')]\n",
    "\n",
    "# Define the number of rows after removing the rows with errors\n",
    "final_rows = df.shape[0]\n",
    "\n",
    "# Print the number of rows removed\n",
    "print(f'{initial_rows - final_rows} rows were removed because their links were broken, led to Error 410')\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('scientists_collab_pids.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-10T03:50:06.057324Z",
     "start_time": "2024-03-10T03:50:06.046698Z"
    }
   },
   "id": "5ee943c44723b2b9",
   "execution_count": 114
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The unique values for the expertise column are: ['{nan, nan}' '{nan, nan, nan}']\n",
      "Because the expertise column has values that are all set of nans, we can set all the values in the expertise column to None\n",
      "\n",
      "For the rows where the \"dblp\" column is a set, we will keep the link that starts with \"https://dblp.org/pid/\" and remove the rest of the links\n",
      "\n",
      "1. 33-6315-1 has set of URLs: {'https://dblp.uni-trier.de/pid/33/6315-1.html', 'https://dblp.org/pid/33/6315-1.html'}\n",
      "Keep this link\": https://dblp.org/pid/33/6315-1.html\n",
      "\n",
      "2. 49-8316 has set of URLs: {'https://dblp.org/pid/49/8316.html', 'https://dblp.org/pers/k/Koutris:Paraschos.html'}\n",
      "Keep this link\": https://dblp.org/pid/49/8316.html\n",
      "\n",
      "3. c-VassilisChristophides has set of URLs: {'https://dblp.org/pid/c/VassilisChristophides.html', 'https://dblp.uni-trier.de/pid/c/VassilisChristophides.html'}\n",
      "Keep this link\": https://dblp.org/pid/c/VassilisChristophides.html\n",
      "\n",
      "4. f-GeorgeHLFletcher has set of URLs: {'https://dblp.org/pid/f/GeorgeHLFletcher.html', 'https://dblp.uni-trier.de/pid/f/GeorgeHLFletcher.html'}\n",
      "Keep this link\": https://dblp.org/pid/f/GeorgeHLFletcher.html\n",
      "\n",
      "5. p-IppokratisPandis has set of URLs: {'https://dblp.uni-trier.de/pid/p/IppokratisPandis.html', 'https://dblp.org/pid/p/IppokratisPandis.html'}\n",
      "Keep this link\": https://dblp.org/pid/p/IppokratisPandis.html\n",
      "\n",
      "6. p-NeoklisPolyzotis has set of URLs: {'https://dblp.org/pid/p/NeoklisPolyzotis.html', 'https://dblp.org/pers/p/Polyzotis:Neoklis.html'}\n",
      "Keep this link\": https://dblp.org/pid/p/NeoklisPolyzotis.html\n",
      "\n",
      "7. p-OdysseasPapapetrou has set of URLs: {'https://dblp.org/pid/p/OdysseasPapapetrou.html', 'https://dblp.org/pers/p/Papapetrou:Odysseas.html'}\n",
      "Keep this link\": https://dblp.org/pid/p/OdysseasPapapetrou.html\n",
      "\n",
      "8. t-AnthonyKHTung has set of URLs: {'https://dblp.org/pid/t/AnthonyKHTung.html', 'https://dblp.org/pers/t/Tung:Anthony_K=_H=.html'}\n",
      "Keep this link\": https://dblp.org/pid/t/AnthonyKHTung.html\n",
      "\n",
      "9. w-StevenEuijongWhang has set of URLs: {'https://dblp.uni-trier.de/pid/w/StevenEuijongWhang.html', 'https://dblp.org/pid/w/StevenEuijongWhang.html'}\n",
      "Keep this link\": https://dblp.org/pid/w/StevenEuijongWhang.html\n",
      "\n",
      "For the rows where the \"final_url\" column is a set, we will keep the link that starts with \"https://dblp.org/pid/\" and remove the rest of the links\n",
      "\n",
      "1. 33-6315-1 has set of URLs: {'https://dblp.uni-trier.de/pid/33/6315-1.html', 'https://dblp.org/pid/33/6315-1.html'}\n",
      "Keep this link\": https://dblp.org/pid/33/6315-1.html\n",
      "\n",
      "2. c-VassilisChristophides has set of URLs: {'https://dblp.org/pid/c/VassilisChristophides.html', 'https://dblp.uni-trier.de/pid/c/VassilisChristophides.html'}\n",
      "Keep this link\": https://dblp.org/pid/c/VassilisChristophides.html\n",
      "\n",
      "3. f-GeorgeHLFletcher has set of URLs: {'https://dblp.org/pid/f/GeorgeHLFletcher.html', 'https://dblp.uni-trier.de/pid/f/GeorgeHLFletcher.html'}\n",
      "Keep this link\": https://dblp.org/pid/f/GeorgeHLFletcher.html\n",
      "\n",
      "4. p-IppokratisPandis has set of URLs: {'https://dblp.uni-trier.de/pid/p/IppokratisPandis.html', 'https://dblp.org/pid/p/IppokratisPandis.html'}\n",
      "Keep this link\": https://dblp.org/pid/p/IppokratisPandis.html\n",
      "\n",
      "5. w-StevenEuijongWhang has set of URLs: {'https://dblp.uni-trier.de/pid/w/StevenEuijongWhang.html', 'https://dblp.org/pid/w/StevenEuijongWhang.html'}\n",
      "Keep this link\": https://dblp.org/pid/w/StevenEuijongWhang.html\n",
      "\n",
      "146 rows were removed because they were duplicates in terms of PIDs\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('scientists_collab_pids.csv')\n",
    "\n",
    "# Define the initial number of rows in the start of this part of the cleaning (before removing duplicates)\n",
    "initial_rows = df.shape[0]\n",
    "\n",
    "# Check if the new cleaned dataset has duplicates in terms of PIDs\n",
    "duplicates = df[df.duplicated(subset='pid', keep=False)]\n",
    "\n",
    "# Sort by the PID\n",
    "duplicates = duplicates.sort_values(by='pid')\n",
    "\n",
    "# For every unique PID, form a row where the pid is unique and the rest of the column are SET of the values for the rows with the same PID\n",
    "duplicates = duplicates.groupby('pid').agg(lambda x: set(x))\n",
    "\n",
    "# If the value for the column is a set of length = 1, get the first element of the set\n",
    "for column in duplicates.columns:\n",
    "    duplicates[column] = duplicates[column].apply(lambda x: x.pop() if len(x) == 1 else x)\n",
    "\n",
    "\n",
    "# Convert the expertise column to a string and print the unique values\n",
    "duplicates['expertise'] = duplicates['expertise'].apply(lambda x: str(x))\n",
    "\n",
    "# Print the unique values for the expertise column\n",
    "print(f'The unique values for the expertise column are: {duplicates[\"expertise\"].unique()}')\n",
    "print('Because the expertise column has values that are all set of nans, we can set all the values in the expertise column to None\\n')\n",
    "\n",
    "# Set every value in the expertise column to None\n",
    "duplicates['expertise'] = None\n",
    "\n",
    "# Ungroup the DataFrame\n",
    "duplicates = duplicates.reset_index()\n",
    "\n",
    "# Remove duplicates from the original DataFrame\n",
    "new_df = df.drop_duplicates(subset='pid', keep=False)\n",
    "\n",
    "# Append the modified duplicates DataFrame to the original DataFrame using concat\n",
    "new_df = pd.concat([new_df, duplicates]).sort_values(by='pid').reset_index(drop=True)\n",
    "\n",
    "new_df = new_df.sort_values(by='pid')\n",
    "\n",
    "print('For the rows where the \"dblp\" column is a set, we will keep the link that starts with \"https://dblp.org/pid/\" and remove the rest of the links\\n')\n",
    "# Iterate over the DataFrame where 'dblp' is a set\n",
    "cnt = 0\n",
    "for index, row in new_df[new_df['dblp'].apply(lambda x: isinstance(x, set))].iterrows():\n",
    "    # For each row, iterate over the set of links\n",
    "    cnt += 1\n",
    "    print(f'{cnt}. {row[\"pid\"]} has set of URLs: {row[\"dblp\"]}')\n",
    "    for link in row['dblp']:\n",
    "        # If a link starts with 'https://dblp.org/pid/', set the 'dblp' value of the row to this link and break the loop\n",
    "        if link.startswith('https://dblp.org/pid/'):\n",
    "            print('Keep this link\": ' + link + '\\n')\n",
    "            new_df.loc[index, 'dblp'] = link\n",
    "            break\n",
    "\n",
    "print('For the rows where the \"final_url\" column is a set, we will keep the link that starts with \"https://dblp.org/pid/\" and remove the rest of the links\\n')\n",
    "# Iterate over the DataFrame where 'final_url' is a set\n",
    "cnt = 0\n",
    "for index, row in new_df[new_df['final_url'].apply(lambda x: isinstance(x, set))].iterrows():\n",
    "    # For each row, iterate over the set of links\n",
    "    cnt += 1\n",
    "    print(f'{cnt}. {row[\"pid\"]} has set of URLs: {row[\"final_url\"]}')\n",
    "    for link in row['final_url']:\n",
    "        # If a link starts with 'https://dblp.org/pid/', set the 'final_url' value of the row to this link and break the loop\n",
    "        if link.startswith('https://dblp.org/pid/'):\n",
    "            print('Keep this link\": ' + link + '\\n')\n",
    "            new_df.loc[index, 'final_url'] = link\n",
    "            break\n",
    "\n",
    "# If there are no duplicates in the new DataFrame, print the message\n",
    "if new_df[new_df.duplicated(subset='pid', keep=False)].shape[0] == 0:\n",
    "    df = new_df\n",
    "    final_rows = df.shape[0]\n",
    "    \n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv('scientists_collab_pids_cleared.csv', index=False)\n",
    "    \n",
    "    # Print the number of rows removed\n",
    "    print(f'{initial_rows - final_rows} rows were removed because they were duplicates in terms of PIDs')\n",
    "else:\n",
    "    print('There are still duplicates in the DataFrame')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-10T03:50:06.082717Z",
     "start_time": "2024-03-10T03:50:06.058186Z"
    }
   },
   "id": "df87776dc725ae8e",
   "execution_count": 115
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the XML links are ending with \".xml\"\n",
      "The XML links have been added to the DataFrame and saved to a CSV file\n"
     ]
    }
   ],
   "source": [
    "# We found that we can extract the XML file associated with each PID by just replacing the '.html' with '.xml' in the URL\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('scientists_collab_pids_cleared.csv')\n",
    "\n",
    "# Creating a new column to hold the XML links\n",
    "df['xml'] = df['final_url'].apply(lambda x: x.replace('.html', '.xml'))\n",
    "\n",
    "# Check if all the XML links are ending with '.xml'\n",
    "if df['xml'].apply(lambda x: x.endswith('.xml')).all():\n",
    "    print('All the XML links are ending with \".xml\"')\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv('scientists_collab_pids_cleared_xml.csv', index=False)\n",
    "    print('The XML links have been added to the DataFrame and saved to a CSV file')\n",
    "else:\n",
    "    print('Not all the XML links are ending with \".xml\"')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-10T03:50:06.092617Z",
     "start_time": "2024-03-10T03:50:06.083357Z"
    }
   },
   "id": "d9cf45958abe11e",
   "execution_count": 116
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1054/1054 [14:31<00:00,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 errors were encountered\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# For each XML link, send a GET request and save the content to a file\n",
    "\n",
    "df = pd.read_csv('scientists_collab_pids_cleared_xml.csv')\n",
    "\n",
    "n_errors = 0\n",
    "\n",
    "# Iterate over the DataFrame rows\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    # Check if the file exists, skip the file if it exists\n",
    "    if f'{row[\"pid\"]}.xml' in os.listdir('xml_files'):\n",
    "        print(f'The file {row[\"pid\"]}.xml already exists')\n",
    "        continue\n",
    "    \n",
    "    while True:\n",
    "        # Send a GET request\n",
    "        response = requests.get(row['xml'])\n",
    "\n",
    "        # If the status code is 429 (Too Many Requests), wait for 60 seconds before trying again\n",
    "        if response.status_code == 429:\n",
    "            print('Too many requests, sleeping for 60 seconds...')\n",
    "            time.sleep(60)\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    # Print Error if the status code is not 200\n",
    "    if response.status_code != 200:\n",
    "        n_errors += 1\n",
    "        print(f'Error: {response.status_code}')\n",
    "        print(f'Error for {row[\"pid\"]}: {row[\"xml\"]}')\n",
    "        continue\n",
    "    \n",
    "    # Save the content to a file (create a new file for each PID)\n",
    "    with open(f'xml_files/{row[\"pid\"]}.xml', 'wb') as file:\n",
    "        file.write(response.content)\n",
    "\n",
    "# Print the number of errors\n",
    "print(f'{n_errors} errors were encountered')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-10T04:04:37.373424Z",
     "start_time": "2024-03-10T03:50:06.093339Z"
    }
   },
   "id": "af438e598d0afe49",
   "execution_count": 117
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Initialize an empty list to store the data for each paper\n",
    "papers = []\n",
    "\n",
    "our_authors = pd.read_csv('scientists_collab_pids_cleared.csv')['pid'].values\n",
    "\n",
    "author_tags = ['author', 'editor']\n",
    "\n",
    "# Iterate over the XML files\n",
    "for filename in os.listdir('xml_files'):\n",
    "    if filename.endswith('.xml'):\n",
    "        # Parse the XML file\n",
    "        tree = ET.parse(os.path.join('xml_files', filename))\n",
    "\n",
    "        # Get the root element\n",
    "        root = tree.getroot()\n",
    "\n",
    "        # Iterate over the <r> elements\n",
    "        for r in root.findall('r'):\n",
    "            # Get the child element of the <r> element\n",
    "            paper = r.find('*')\n",
    "            if paper is not None:\n",
    "                # Extract the title, year, doi, and pids of all authors\n",
    "                title = paper.find('title').text if paper.find('title') is not None else None\n",
    "                if title is None:\n",
    "                    title = ET.tostring(paper.find('title'))\n",
    "                paper_key = paper.get('key') if paper.get('key') is not None else None\n",
    "                year = paper.find('year').text if paper.find('year') is not None else None\n",
    "                doi = paper.find('ee').text if paper.find('ee') is not None else None\n",
    "                authors = []\n",
    "                for a_tag in author_tags:\n",
    "                    authors.extend([author.get('pid').replace('/', '-') for author in paper.findall(a_tag) if author.get('pid').replace('/', '-') in our_authors])\n",
    "\n",
    "                # Check if the title, year, and doi are not None\n",
    "                if title is not None and year is not None and len(authors) > 0:\n",
    "                    # Append the extracted data to the list\n",
    "                    papers.append([title, year, doi, authors, filename])\n",
    "                else:\n",
    "                    print(f'Error: {filename}, {paper_key} --> title: {title}, year: {year}, doi: {doi}, authors: {authors}')\n",
    "            else:\n",
    "                print(f'Error: {filename}, {paper}')\n",
    "\n",
    "# Convert the list to a pandas DataFrame\n",
    "df = pd.DataFrame(papers, columns=['Title', 'Year', 'DOI', 'Authors', 'file'])\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('papers.csv', index=False)\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-10T05:04:27.424531Z",
     "start_time": "2024-03-10T05:04:17.942977Z"
    }
   },
   "id": "3616d0d69be43211",
   "execution_count": 151
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique papers is: 93827\n",
      "The number of unique years is: 52\n",
      "The number of unique DOIs is: 99375\n",
      "The number of unique files is: 1054\n",
      "The number of papers with missing DOIs is: 2624\n",
      "The number of papers with missing titles is: 0\n",
      "The number of papers with missing years is: 0\n",
      "The number of papers with missing authors is: 0\n",
      "(129096, 5)\n"
     ]
    }
   ],
   "source": [
    "# Perform analysis on the papers DataFrame\n",
    "df = pd.read_csv('papers.csv')\n",
    "\n",
    "# Print the number of unique papers\n",
    "print(f'The number of unique papers is: {df[\"Title\"].nunique()}')\n",
    "\n",
    "# Print the number of unique years\n",
    "print(f'The number of unique years is: {df[\"Year\"].nunique()}')\n",
    "\n",
    "# Print the number of unique DOIs\n",
    "print(f'The number of unique DOIs is: {df[\"DOI\"].nunique()}')\n",
    "\n",
    "# Print the number of unique files\n",
    "print(f'The number of unique files is: {df[\"file\"].nunique()}')\n",
    "\n",
    "# Print the number of papers with missing DOIs\n",
    "print(f'The number of papers with missing DOIs is: {df[\"DOI\"].isna().sum()}')\n",
    "\n",
    "# Print the number of papers with missing titles\n",
    "print(f'The number of papers with missing titles is: {df[\"Title\"].isna().sum()}')\n",
    "\n",
    "# Print the number of papers with missing years\n",
    "print(f'The number of papers with missing years is: {df[\"Year\"].isna().sum()}')\n",
    "\n",
    "# Print the number of papers with missing authors\n",
    "print(f'The number of papers with missing authors is: {df[\"Authors\"].apply(lambda x: len(x) == 0).sum()}')\n",
    "\n",
    "# Print the shape of the DataFrame\n",
    "print(df.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-10T05:08:21.159923Z",
     "start_time": "2024-03-10T05:08:20.883688Z"
    }
   },
   "id": "748046266f8c7ddd",
   "execution_count": 154
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df = pd.read_csv('papers.csv')\n",
    "\n",
    "# Remove the duplicates from the DataFrame\n",
    "df = df.drop_duplicates(subset=['Title', 'Year', 'DOI', 'Authors'])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('papers_cleaned.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-10T05:12:34.357502Z",
     "start_time": "2024-03-10T05:12:33.928046Z"
    }
   },
   "id": "44fa430b62cf4ee4",
   "execution_count": 156
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique papers is: 93827\n",
      "The number of unique years is: 52\n",
      "The number of unique DOIs is: 99375\n",
      "The number of unique files is: 1049\n",
      "The number of papers with missing DOIs is: 2236\n",
      "The number of papers with missing titles is: 0\n",
      "The number of papers with missing years is: 0\n",
      "The number of papers with missing authors is: 0\n",
      "(101698, 5)\n"
     ]
    }
   ],
   "source": [
    "# Perform analysis on the papers DataFrame\n",
    "df = pd.read_csv('papers_cleaned.csv')\n",
    "\n",
    "# Print the number of unique papers\n",
    "print(f'The number of unique papers is: {df[\"Title\"].nunique()}')\n",
    "\n",
    "# Print the number of unique years\n",
    "print(f'The number of unique years is: {df[\"Year\"].nunique()}')\n",
    "\n",
    "# Print the number of unique DOIs\n",
    "print(f'The number of unique DOIs is: {df[\"DOI\"].nunique()}')\n",
    "\n",
    "# Print the number of unique files\n",
    "print(f'The number of unique files is: {df[\"file\"].nunique()}')\n",
    "\n",
    "# Print the number of papers with missing DOIs\n",
    "print(f'The number of papers with missing DOIs is: {df[\"DOI\"].isna().sum()}')\n",
    "\n",
    "# Print the number of papers with missing titles\n",
    "print(f'The number of papers with missing titles is: {df[\"Title\"].isna().sum()}')\n",
    "\n",
    "# Print the number of papers with missing years\n",
    "print(f'The number of papers with missing years is: {df[\"Year\"].isna().sum()}')\n",
    "\n",
    "# Print the number of papers with missing authors\n",
    "print(f'The number of papers with missing authors is: {df[\"Authors\"].apply(lambda x: len(x) == 0).sum()}')\n",
    "\n",
    "# Print the shape of the DataFrame\n",
    "print(df.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-10T05:12:48.903177Z",
     "start_time": "2024-03-10T05:12:48.699311Z"
    }
   },
   "id": "1f26111c6bdc7036",
   "execution_count": 158
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4b24b9b5d89a8460"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
